
@inproceedings{rahman_fairwalk_2019,
	address = {Macao, China},
	title = {Fairwalk: {Towards} {Fair} {Graph} {Embedding}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Fairwalk},
	url = {https://www.ijcai.org/proceedings/2019/456},
	doi = {10.24963/ijcai.2019/456},
	abstract = {Graph embeddings have gained huge popularity in the recent years as a powerful tool to analyze social networks. However, no prior works have studied potential bias issues inherent within graph embedding. In this paper, we make a ﬁrst attempt in this direction. In particular, we concentrate on the fairness of node2vec, a popular graph embedding method. Our analyses on two real-world datasets demonstrate the existence of bias in node2vec when used for friendship recommendation. We therefore propose a fairness-aware embedding method, namely Fairwalk, which extends node2vec. Experimental results demonstrate that Fairwalk reduces bias under multiple fairness metrics while still preserving the utility.},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Rahman, Tahleen and Surma, Bartlomiej and Backes, Michael and Zhang, Yang},
	month = aug,
	year = {2019},
	pages = {3289--3295},
	file = {Rahman et al. - 2019 - Fairwalk Towards Fair Graph Embedding.pdf:/home/ashutosh/Zotero/storage/Z29TCK5E/Rahman et al. - 2019 - Fairwalk Towards Fair Graph Embedding.pdf:application/pdf},
}

@article{khajehnejad_crosswalk_2021,
	title = {{CrossWalk}: {Fairness}-enhanced {Node} {Representation} {Learning}},
	shorttitle = {{CrossWalk}},
	url = {https://arxiv.org/abs/2105.02725v2},
	doi = {10.48550/arXiv.2105.02725},
	abstract = {The potential for machine learning systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. Much recent work has focused on developing algorithmic tools to assess and mitigate such unfairness. However, there is little work on enhancing fairness in graph algorithms. Here, we develop a simple, effective and general method, CrossWalk, that enhances fairness of various graph algorithms, including influence maximization, link prediction and node classification, applied to node embeddings. CrossWalk is applicable to any random walk based node representation learning algorithm, such as DeepWalk and Node2Vec. The key idea is to bias random walks to cross group boundaries, by upweighting edges which (1) are closer to the groups' peripheries or (2) connect different groups in the network. CrossWalk pulls nodes that are near groups' peripheries towards their neighbors from other groups in the embedding space, while preserving the necessary structural properties of the graph. Extensive experiments show the effectiveness of our algorithm to enhance fairness in various graph algorithms, including influence maximization, link prediction and node classification in synthetic and real networks, with only a very small decrease in performance.},
	language = {en},
	urldate = {2022-07-29},
	author = {Khajehnejad, Ahmad and Khajehnejad, Moein and Babaei, Mahmoudreza and Gummadi, Krishna P. and Weller, Adrian and Mirzasoleiman, Baharan},
	month = may,
	year = {2021},
	file = {Full Text PDF:/home/ashutosh/Zotero/storage/TGRH7MDG/Khajehnejad et al. - 2021 - CrossWalk Fairness-enhanced Node Representation L.pdf:application/pdf;Snapshot:/home/ashutosh/Zotero/storage/F54CHELK/2105.html:text/html},
}

@inproceedings{laclau_all_2021,
	title = {All of the {Fairness} for {Edge} {Prediction} with {Optimal} {Transport}},
	url = {https://proceedings.mlr.press/v130/laclau21a.html},
	abstract = {Machine learning and data mining algorithms have been increasingly used recently to support decision-making systems in many areas of high societal importance such as healthcare, education, or security. While being very efficient in their predictive abilities, the deployed algorithms sometimes tend to learn an inductive model with a discriminative bias due to the presence of this latter in the learning sample. This problem gave rise to a new field of algorithmic fairness where the goal is to correct the discriminative bias introduced by a certain attribute in order to decorrelate it from the model’s output. In this paper, we study the problem of fairness for the task of edge prediction in graphs, a largely underinvestigated scenario compared to a more popular setting of fair classification. To this end, we formulate the problem of fair edge prediction, analyze it theoretically, and propose an embedding-agnostic repairing procedure for the adjacency matrix of an arbitrary graph with a trade-off between the group and individual fairness. We experimentally show the versatility of our approach and its capacity to provide explicit control over different notions of fairness and prediction accuracy.},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Laclau, Charlotte and Redko, Ievgen and Choudhary, Manvi and Largeron, Christine},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1774--1782},
	file = {Full Text PDF:/home/ashutosh/Zotero/storage/F9RI4E7V/Laclau et al. - 2021 - All of the Fairness for Edge Prediction with Optim.pdf:application/pdf;Supplementary PDF:/home/ashutosh/Zotero/storage/X5WGG4U4/Laclau et al. - 2021 - All of the Fairness for Edge Prediction with Optim.pdf:application/pdf},
}

@misc{gonen_lipstick_2019,
	title = {Lipstick on a {Pig}: {Debiasing} {Methods} {Cover} up {Systematic} {Gender} {Biases} in {Word} {Embeddings} {But} do not {Remove} {Them}},
	shorttitle = {Lipstick on a {Pig}},
	url = {http://arxiv.org/abs/1903.03862},
	doi = {10.48550/arXiv.1903.03862},
	abstract = {Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between "gender-neutralized" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Gonen, Hila and Goldberg, Yoav},
	month = sep,
	year = {2019},
	note = {arXiv:1903.03862 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to NAACL 2019},
	file = {arXiv Fulltext PDF:/home/ashutosh/Zotero/storage/ZCHKKM3H/Gonen and Goldberg - 2019 - Lipstick on a Pig Debiasing Methods Cover up Syst.pdf:application/pdf;arXiv.org Snapshot:/home/ashutosh/Zotero/storage/YCE7XXMA/1903.html:text/html},
}
@article{DBLP:journals/corr/PerozziAS14,
  author    = {Bryan Perozzi and
               Rami Al{-}Rfou and
               Steven Skiena},
  title     = {DeepWalk: Online Learning of Social Representations},
  journal   = {CoRR},
  volume    = {abs/1403.6652},
  year      = {2014},
  url       = {http://arxiv.org/abs/1403.6652},
  eprinttype = {arXiv},
  eprint    = {1403.6652},
  timestamp = {Thu, 30 Apr 2020 11:17:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PerozziAS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{kenna_using_2021,
	title = {Using {Adversarial} {Debiasing} to {Remove} {Bias} from {Word} {Embeddings}},
	url = {http://arxiv.org/abs/2107.10251},
	doi = {10.48550/arXiv.2107.10251},
	abstract = {Word Embeddings have been shown to contain the societal biases present in the original corpora. Existing methods to deal with this problem have been shown to only remove superficial biases. The method of Adversarial Debiasing was presumed to be similarly superficial, but this is was not verified in previous works. Using the experiments that demonstrated the shallow removal in other methods, I show results that suggest Adversarial Debiasing is more effective at removing bias and thus motivate further investigation on the utility of Adversarial Debiasing.},
	urldate = {2022-07-01},
	publisher = {arXiv},
	author = {Kenna, Dana},
	month = jul,
	year = {2021},
	note = {arXiv:2107.10251 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages, 9 Figures - Adapted from my thesis project},
	file = {arXiv Fulltext PDF:/home/ashutosh/Zotero/storage/HIUW8QNH/Kenna - 2021 - Using Adversarial Debiasing to Remove Bias from Wo.pdf:application/pdf;arXiv.org Snapshot:/home/ashutosh/Zotero/storage/VYEA5YZP/2107.html:text/html},
}

@techreport{brunet_understanding_2019,
	title = {Understanding the {Origins} of {Bias} in {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1810.03611},
	abstract = {The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes in important contexts. Although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations, there is a lack of understanding in how word embedding bias depends on the training data. In this work, we develop a technique for understanding the origins of bias in word embeddings. Given a word embedding trained on a corpus, our method identifies how perturbing the corpus will affect the bias of the resulting embedding. This can be used to trace the origins of word embedding bias back to the original training documents. Using our method, one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias. We demonstrate our techniques on both a New York Times and Wikipedia corpus and find that our influence function-based approximations are very accurate.},
	number = {arXiv:1810.03611},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
	month = jun,
	year = {2019},
	doi = {10.48550/arXiv.1810.03611},
	note = {arXiv:1810.03611 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/home/ashutosh/Zotero/storage/5NP8V8H9/Brunet et al. - 2019 - Understanding the Origins of Bias in Word Embeddin.pdf:application/pdf;arXiv.org Snapshot:/home/ashutosh/Zotero/storage/HUCYAPY6/1810.html:text/html},
}

@article{kojaku_residual2vec_2021,
	title = {{Residual2Vec}: {Debiasing} graph embedding with random graphs},
	shorttitle = {{Residual2Vec}},
	url = {https://www.semanticscholar.org/reader/25eedb8a3d68859d37efc114ce9b666ec5d78b3e},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	language = {en},
	urldate = {2022-05-22},
	journal = {undefined},
	author = {Kojaku, Sadamori and Yoon, Jisung and Constantino, I. and Ahn, Yong-Yeol},
	year = {2021},
	file = {Full Text PDF:/home/ashutosh/Zotero/storage/ELDERR2S/Kojaku et al. - 2021 - Residual2Vec Debiasing graph embedding with rando.pdf:application/pdf;Snapshot:/home/ashutosh/Zotero/storage/H3HP4IM3/25eedb8a3d68859d37efc114ce9b666ec5d78b3e.html:text/html},
}

@article{garg_word_2018,
	title = {Word {Embeddings} {Quantify} 100 {Years} of {Gender} and {Ethnic} {Stereotypes}},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1711.08412},
	doi = {10.1073/pnas.1720347115},
	abstract = {Word embeddings use vectors to represent words such that the geometry between vectors captures semantic relationship between the words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding can be leveraged to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 years of text data with the U.S. Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures global social shifts -- e.g., the women's movement in the 1960s and Asian immigration into the U.S -- and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a powerful new intersection between machine learning and quantitative social science.},
	number = {16},
	urldate = {2022-07-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	month = apr,
	year = {2018},
	note = {arXiv:1711.08412 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/home/ashutosh/Zotero/storage/HUMFD3K3/Garg et al. - 2018 - Word Embeddings Quantify 100 Years of Gender and E.pdf:application/pdf;arXiv.org Snapshot:/home/ashutosh/Zotero/storage/35MS4HZI/1711.html:text/html},
}

@misc{ravfogel_null_2020,
	title = {Null {It} {Out}: {Guarding} {Protected} {Attributes} by {Iterative} {Nullspace} {Projection}},
	shorttitle = {Null {It} {Out}},
	url = {http://arxiv.org/abs/2004.07667},
	doi = {10.48550/arXiv.2004.07667},
	abstract = {The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
	month = apr,
	year = {2020},
	note = {arXiv:2004.07667 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted as a long paper in ACL 2020},
	file = {arXiv Fulltext PDF:/home/ashutosh/Zotero/storage/JV2RW7IF/Ravfogel et al. - 2020 - Null It Out Guarding Protected Attributes by Iter.pdf:application/pdf;arXiv.org Snapshot:/home/ashutosh/Zotero/storage/BJTT4KLM/2004.html:text/html},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	volume = {29},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	urldate = {2022-06-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year = {2016},
	file = {Full Text PDF:/home/ashutosh/Zotero/storage/2BLXFC6A/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf},
}
