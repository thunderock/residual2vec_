{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 22:52:44.734925: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 22:52:44.735086: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-29 22:52:44.736111: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepWalk\n",
      "residual2vec_sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [00:00<00:00, 440.29it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual2vec_sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [00:00<00:00, 475.72it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairwalk\n",
      "Fairwalk\n",
      "GCN\n",
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff1f9b01990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff1f9b01990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff1f9b49b50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff1f9b49b50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 22:52:51.304812: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-29 22:52:51.325038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2304000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0086 - val_loss: 0.0105\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0083 - val_loss: 0.0108\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0079 - val_loss: 0.0109\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0076 - val_loss: 0.0112\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0077 - val_loss: 0.0113\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0077 - val_loss: 0.0113\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0076 - val_loss: 0.0111\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0074 - val_loss: 0.0110\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0074 - val_loss: 0.0109\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0077 - val_loss: 0.0109\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0073 - val_loss: 0.0110\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0073 - val_loss: 0.0111\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0069 - val_loss: 0.0115\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0071 - val_loss: 0.0116\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0068 - val_loss: 0.0116\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0069 - val_loss: 0.0116\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0065 - val_loss: 0.0116\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0065 - val_loss: 0.0117\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0067 - val_loss: 0.0117\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0068 - val_loss: 0.0117\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0066 - val_loss: 0.0117\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0064 - val_loss: 0.0116\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0065 - val_loss: 0.0115\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0061 - val_loss: 0.0115\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0062 - val_loss: 0.0115\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0062 - val_loss: 0.0116\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0063 - val_loss: 0.0116\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0067 - val_loss: 0.0116\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0062 - val_loss: 0.0117\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0062 - val_loss: 0.0118\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0060 - val_loss: 0.0119\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0059 - val_loss: 0.0119\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0058 - val_loss: 0.0120\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0059 - val_loss: 0.0124\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0057 - val_loss: 0.0123\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0058 - val_loss: 0.0122\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0058 - val_loss: 0.0118\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0057 - val_loss: 0.0118\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0058 - val_loss: 0.0120\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0057 - val_loss: 0.0121\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0056 - val_loss: 0.0123\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0062 - val_loss: 0.0126\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0055 - val_loss: 0.0128\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0054 - val_loss: 0.0130\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0053 - val_loss: 0.0130\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0055 - val_loss: 0.0131\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0053 - val_loss: 0.0131\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0052 - val_loss: 0.0132\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0055 - val_loss: 0.0129\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0052 - val_loss: 0.0126\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0050 - val_loss: 0.0124\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0055 - val_loss: 0.0124\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0055 - val_loss: 0.0122\n",
      "GCN\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 737ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0082 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0080 - val_loss: 0.0107\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0078 - val_loss: 0.0107\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0076 - val_loss: 0.0107\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0078 - val_loss: 0.0108\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0076 - val_loss: 0.0109\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0075 - val_loss: 0.0110\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0076 - val_loss: 0.0111\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0076 - val_loss: 0.0111\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0074 - val_loss: 0.0111\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0071 - val_loss: 0.0112\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0074 - val_loss: 0.0114\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0073 - val_loss: 0.0113\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0072 - val_loss: 0.0112\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0069 - val_loss: 0.0112\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0071 - val_loss: 0.0111\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0069 - val_loss: 0.0112\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0069 - val_loss: 0.0114\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0068 - val_loss: 0.0114\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0067 - val_loss: 0.0115\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0066 - val_loss: 0.0114\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0067 - val_loss: 0.0114\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0066 - val_loss: 0.0114\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0067 - val_loss: 0.0114\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0067 - val_loss: 0.0115\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0067 - val_loss: 0.0115\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0065 - val_loss: 0.0116\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0064 - val_loss: 0.0117\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0062 - val_loss: 0.0118\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0064 - val_loss: 0.0118\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0062 - val_loss: 0.0118\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0062 - val_loss: 0.0118\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0061 - val_loss: 0.0118\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0066 - val_loss: 0.0118\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0059 - val_loss: 0.0119\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0061 - val_loss: 0.0118\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0059 - val_loss: 0.0117\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0061 - val_loss: 0.0117\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0057 - val_loss: 0.0118\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0057 - val_loss: 0.0120\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0057 - val_loss: 0.0122\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0058 - val_loss: 0.0122\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0053 - val_loss: 0.0123\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0056 - val_loss: 0.0123\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0055 - val_loss: 0.0123\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0053 - val_loss: 0.0123\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0058 - val_loss: 0.0124\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0057 - val_loss: 0.0124\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0059 - val_loss: 0.0124\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0054 - val_loss: 0.0125\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0052 - val_loss: 0.0126\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0055 - val_loss: 0.0125\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0054 - val_loss: 0.0124\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0055 - val_loss: 0.0122\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0056 - val_loss: 0.0121\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0053 - val_loss: 0.0122\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0056 - val_loss: 0.0124\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0054 - val_loss: 0.0124\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0053 - val_loss: 0.0123\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0055 - val_loss: 0.0123\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0049 - val_loss: 0.0124\n",
      "GraphSage\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff2187d2310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff2187d2310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff218798190>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff218798190>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7150 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.7133 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 0.6881 - binary_accuracy: 0.5095\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.6611 - binary_accuracy: 0.5619\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.6449 - binary_accuracy: 0.6381\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "GraphSage\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7262 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.7080 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 0.6756 - binary_accuracy: 0.5048\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 0.6492 - binary_accuracy: 0.6000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.6538 - binary_accuracy: 0.5952\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "GAT\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff1a00293d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff1a00293d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0081 - val_loss: 0.0103\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff1f9f99290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "GAT\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0097 - val_loss: 0.0100\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0095 - val_loss: 0.0100\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0089 - val_loss: 0.0102\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0097 - val_loss: 0.0102\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0096 - val_loss: 0.0102\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff1d009c5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "<class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "import graph_tool.all as gt\n",
    "import sys, os\n",
    "import graph_embeddings\n",
    "from utils.score import statistical_parity\n",
    "import faiss\n",
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "DATA_FILE = 'data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "\n",
    "models = {}\n",
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "\n",
    "models[\"unbiased\"] = graph_embeddings.DeepWalk(window_length=window_length, num_walks=num_walks, restart_prob=0)\n",
    "\n",
    "models[\"degree-unbiased\"] = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    cuda=True,\n",
    "    walk_length=80\n",
    ")\n",
    "\n",
    "models[\"group-unbiased\"] = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    cuda=True,\n",
    "    walk_length=80,\n",
    ")\n",
    "\n",
    "models[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks)\n",
    "models[\"fairwalk-group-unbiased\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ")\n",
    "models['GCN'] = graph_embeddings.GCN()\n",
    "models[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2)\n",
    "models[\"graphsage\"] = graph_embeddings.GraphSage()\n",
    "models[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2)\n",
    "models[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256])\n",
    "models[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2)\n",
    "embs = {}\n",
    "for k, model in models.items():\n",
    "    print(model.__class__.__name__)\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    emb = model.fit(A).transform(dim=dim)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    embs[k] = emb\n",
    "\n",
    "\n",
    "def reconstruct_graph(emb, n, m):\n",
    "    # choose top m edges to reconstruct the graph\n",
    "    S = emb @ emb.T\n",
    "    S = np.triu(S, k=1)\n",
    "    r, c, v = sparse.find(S)\n",
    "    idx = np.argsort(-v)[:m]\n",
    "    r, c, v = r[idx], c[idx], v[idx]\n",
    "    B = sparse.csr_matrix((v, (r, c)), shape=(n, n))\n",
    "    B = B + B.T\n",
    "    B.data = B.data * 0 + 1\n",
    "    return nx.from_scipy_sparse_matrix(B + B.T)\n",
    "\n",
    "n_edges = int(A.sum() / 2)\n",
    "n_nodes = A.shape[0]\n",
    "rgraphs = {}\n",
    "for k, emb in embs.items():\n",
    "    rgraphs[k] = reconstruct_graph(emb, n_nodes, n_edges)\n",
    "    print(type(rgraphs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2409.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  unbiased 0.1464518608790001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2252.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  degree-unbiased 0.15604094802988733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2871.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  group-unbiased 0.10059361653570019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2455.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  fairwalk 0.1488161993482154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3033.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  fairwalk-group-unbiased 0.14965895096260692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2572.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  GCN 0.13534382882098314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3206.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  gcn-doubleK 0.17245112321517442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3929.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  graphsage 0.08425043288877315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3029.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  graphsage-doubleK 0.09101280073146133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3574.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  gat 0.10835415863751031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3974.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  gat-doubleK 0.07656589674549143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for k, graph in rgraphs.items():\n",
    "    scores[k] = statistical_parity(graph, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[Text(0,0,'unbiased'),\n Text(0,0,'degree-unbiased'),\n Text(0,0,'group-unbiased'),\n Text(0,0,'fairwalk'),\n Text(0,0,'fairwalk-group-unbiased'),\n Text(0,0,'GCN'),\n Text(0,0,'gcn-doubleK'),\n Text(0,0,'graphsage'),\n Text(0,0,'graphsage-doubleK'),\n Text(0,0,'gat'),\n Text(0,0,'gat-doubleK')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}