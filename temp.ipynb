{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21c0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('/tmp/cora/cora.content',sep = '\\t',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf281e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1050679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1103960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>1103985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>1109199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>1112911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>1113438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>1113831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0        1\n",
       "0  35     1033\n",
       "1  35   103482\n",
       "2  35   103515\n",
       "3  35  1050679\n",
       "4  35  1103960\n",
       "5  35  1103985\n",
       "6  35  1109199\n",
       "7  35  1112911\n",
       "8  35  1113438\n",
       "9  35  1113831"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_cites = pd.read_csv('/tmp/cora/cora.cites',sep = '\\t',header = None)\n",
    "raw_data_cites[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742d6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    Turn the label into a onehot vector\n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    print(classes_dict)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd6bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"\n",
    "    Normalize sparse matrix by row\n",
    "    \"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e60ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    Convert a sparse matrix from scipy format to torch format\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c9b190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"/tmp/cora/\", dataset=\"cora\"):\n",
    "    \n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    \n",
    "    ## content data is converted to numpy vector\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    \n",
    "    # Take the bag-of-words vector of each paper as the feature vector of each article and store it in a sparse matrix format\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    # Take the type of each paper as a label and convert it into a one hot vector\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # # Take out the id of each paper\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    \n",
    "    # cites data is converted to numpy vector\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    \n",
    "    # Map the id in the cites data to the interval [0, 2708]\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # Store the citation relationship between papers in a sparse matrix format\n",
    "    # 1, 0 matrix\n",
    "    adj_v = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj_v = adj_v + adj_v.T.multiply(adj_v.T > adj_v) - adj_v.multiply(adj_v.T > adj_v)\n",
    "    \n",
    "    # Normalize the characteristics of the article\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj_v + sp.eye(adj_v.shape[0]))\n",
    "    \n",
    "    # Produce the final vector\n",
    "    idx_train = range(500)\n",
    "    idx_val = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "\n",
    "    return adj_v, adj, features, labels, idx_train, idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cca0bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "{'Genetic_Algorithms': array([1., 0., 0., 0., 0., 0., 0.]), 'Case_Based': array([0., 1., 0., 0., 0., 0., 0.]), 'Neural_Networks': array([0., 0., 1., 0., 0., 0., 0.]), 'Probabilistic_Methods': array([0., 0., 0., 1., 0., 0., 0.]), 'Rule_Learning': array([0., 0., 0., 0., 1., 0., 0.]), 'Reinforcement_Learning': array([0., 0., 0., 0., 0., 1., 0.]), 'Theory': array([0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]),\n",
       " torch.Tensor,\n",
       " torch.Size([2708]),\n",
       " torch.Tensor,\n",
       " torch.Size([2708, 2708]),\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_v, adj, features, labels, idx_train, idx_val = load_data()\n",
    "features.shape, type(features), labels.shape, type(labels), adj.shape, type(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6543d107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 128]),\n",
       " torch.Tensor,\n",
       " torch.Size([105]),\n",
       " torch.Tensor,\n",
       " torch.Size([105, 105]),\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "\n",
    "t = 'fairwalk'\n",
    "embs = pkl.load(open('/tmp/embs.pkl', 'rb'))\n",
    "rgraphs = pkl.load(open(\"/tmp/rgraphs.pkl\", 'rb'))\n",
    "labels = torch.LongTensor(pkl.load(open(\"/tmp/group_ids.pkl\", 'rb')))\n",
    "adj = nx.to_scipy_sparse_matrix(rgraphs[t])\n",
    "adj = sparse_mx_to_torch_sparse_tensor(normalize(adj + sp.eye(adj.shape[0])))\n",
    "features = torch.FloatTensor(embs[t])\n",
    "idx_train, idx_val = torch.LongTensor(range(80)), torch.LongTensor(range(80, 105))\n",
    "features.shape, type(features), labels.shape, type(labels), adj.shape, type(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b4d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8219c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860d3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "     A simple implementation of graph convolution, please refer to the paper https://arxiv.org/abs/1609.02907\n",
    "     ...\n",
    "     Attributes\n",
    "     ----------\n",
    "     in_features: int\n",
    "         The size of the image convolution input feature vector, namely $|H^{(l)}|$\n",
    "     out_features: int\n",
    "         The size of the image convolution output vector, namely $|H^{(l+1)}|$\n",
    "     bias: bool\n",
    "         Whether to use the offset vector, the default is True, that is, the default is to use the offset vector\n",
    "     weight: Parameter\n",
    "         Trainable parameters in graph convolution,\n",
    "        \n",
    "     Methods\n",
    "     -------\n",
    "     __init__(self, in_features, out_features, bias=True)\n",
    "         The constructor of the graph convolution, defines the size of the input feature, the size of the output vector, whether to use offset, parameters\n",
    "     reset_parameters(self)\n",
    "         Initialize the parameters in the graph convolution\n",
    "     forward(self, input, adj)\n",
    "         Forward propagation function, input is the feature input, and adj is the transformed adjacency matrix $N(A)=D^{-1}\\tilde{A}$. Completing the calculation logic of forward propagation, $N(A) H^{(l)} W^{(l)}$\n",
    "     __repr__(self)\n",
    "         Refactored class name expression\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        \n",
    "        # create Weight and Bias trainable parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # standard weight to be uniform\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # H * W\n",
    "        support = torch.mm(input, self.weight)\n",
    "        \n",
    "        # N(A) * H * W # Addition aggregation by multiplying\n",
    "        output = torch.spmm(adj, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            # N(A) * H * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0a574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c1438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "     Multiple graph convolutional neural network model\n",
    "     ...\n",
    "     Attributes\n",
    "     ----------\n",
    "     n_feat: int\n",
    "         The size of the input feature vector of the graph network\n",
    "     n_hid: int\n",
    "         The size of the hidden layer, that is, the size of the output vector of the first layer of the convolutional layer\n",
    "     n_class: int\n",
    "         Number of classifier categories\n",
    "     dropout: float\n",
    "         dropout rate\n",
    "        \n",
    "     Methods\n",
    "     -------\n",
    "     __init__(self, n_feat, n_hid, n_class, dropout)\n",
    "         Two-layer graph convolutional neural network constructor, defining the dimension of the input feature, the dimension of the hidden layer, the number of classifier categories, and the dropout rate\n",
    "     forward(self, x, adj)\n",
    "         Forward propagation function, x is the input feature of the graph network, adj is the adjacency matrix that has been transformed $N(A)$\n",
    "     '''\n",
    "    def __init__(self, n_feat,n_hids, n_class, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # Define the layers of graph convolutional layer\n",
    "        \n",
    "        layers_units = [n_feat] +  n_hids\n",
    "        \n",
    "        self.graph_layers = nn.ModuleList([GraphConvolution(layers_units[idx],layers_units[idx+1]) for idx in range(len(layers_units)-1)])\n",
    "        \n",
    "        self.output_layer = GraphConvolution(layers_units[-1],n_class)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        for graph_layer in self.graph_layers:\n",
    "            x = F.relu(graph_layer(x, adj))\n",
    "            # dropout\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            \n",
    "        # The output of the final convolutional layer is mapped to the output category dimension\n",
    "        x = self.output_layer(x, adj)\n",
    "        \n",
    "        # Calculate log softmax\n",
    "        # https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386/20\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71800e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    \"\"\"\n",
    "    Accuracy calculation method\n",
    "    \"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7131d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Training hyperparameter configuration\n",
    "class Args:\n",
    "    no_cuda = False # Whether to use cuda/gpu\n",
    "    seed = 42 # Set random seed\n",
    "    epochs = 200 # number of iterations\n",
    "    lr = 0.01 # learning rate\n",
    "    weight_decay = 5e-4 # Learning rate decay\n",
    "    n_hid = 16 # hidden layer dimension\n",
    "    dropout = 0.5 # dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2bab89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6daac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ef326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = features.shape[1]\n",
    "model = GCN(n_feat=input_features,\n",
    "            n_hids=[128,32],\n",
    "            n_class=labels.max().item() + 1,\n",
    "            dropout=args.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7b5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an Adam optimizer,\n",
    "# The parameters to be optimized are the trainable parameters in the GCN model\n",
    "# Learning rate is set to args.lr\n",
    "# Learning rate decay is args.weight_decay\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80851729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"\n",
    "    Accuracy calculation method\n",
    "    \"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27afd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function, epoch is the number of iterations\n",
    "def train(epoch):\n",
    "    # Record the start time of the epoch iteration\n",
    "    t = time.time()\n",
    "    # Mark the GCN model is in train mode\n",
    "    model.train()\n",
    "    # In each epoch, you need to clear the previously calculated gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \"\"\"\n",
    "    Input the graph network input feature and the transformed adjacency matrix adj into the graph convolutional neural network GCN model, and the output is obtained through forward propagation, \n",
    "    which is the predicted probability of the classification category\n",
    "    \"\"\"\n",
    "    \n",
    "    output = model(features, adj)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Find the corresponding output probability and label according to the data index of the training set, \n",
    "    and then calculate the loss and accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    # # Error back propagation\n",
    "    loss_train.backward()\n",
    "    \n",
    "    # The optimizer starts to optimize the trainable parameters in GCN\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    \"\"\"Use the validation set data to verify the epoch training results. \n",
    "    The verification process needs to close the train mode and open the eval model\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # # Same forward propagation\n",
    "    output = model(features, adj)\n",
    "    \"\"\"\n",
    "    Find the corresponding output probability and label according to the data index of the validation set,\n",
    "    and then calculate the loss and accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    # Print all the results and the time required\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d428a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.9695 acc_train: 0.6125 loss_val: 0.8638 acc_val: 0.8800 time: 0.0032s\n",
      "Epoch: 0002 loss_train: 0.7781 acc_train: 0.7625 loss_val: 0.5636 acc_val: 0.9200 time: 0.0018s\n",
      "Epoch: 0003 loss_train: 0.6431 acc_train: 0.8250 loss_val: 0.3377 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0004 loss_train: 0.4924 acc_train: 0.8250 loss_val: 0.2369 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0005 loss_train: 0.4457 acc_train: 0.8375 loss_val: 0.2031 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0006 loss_train: 0.3984 acc_train: 0.8375 loss_val: 0.1757 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0007 loss_train: 0.4595 acc_train: 0.8375 loss_val: 0.1661 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0008 loss_train: 0.3790 acc_train: 0.8125 loss_val: 0.1641 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0009 loss_train: 0.3260 acc_train: 0.8375 loss_val: 0.1636 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0010 loss_train: 0.3244 acc_train: 0.8125 loss_val: 0.1728 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0011 loss_train: 0.3453 acc_train: 0.8125 loss_val: 0.1892 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0012 loss_train: 0.4083 acc_train: 0.8250 loss_val: 0.1960 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0013 loss_train: 0.3121 acc_train: 0.8500 loss_val: 0.1926 acc_val: 0.9200 time: 0.0018s\n",
      "Epoch: 0014 loss_train: 0.2814 acc_train: 0.8750 loss_val: 0.1848 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0015 loss_train: 0.3103 acc_train: 0.8625 loss_val: 0.1831 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0016 loss_train: 0.3529 acc_train: 0.8625 loss_val: 0.1800 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0017 loss_train: 0.2978 acc_train: 0.9250 loss_val: 0.1857 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0018 loss_train: 0.3319 acc_train: 0.8500 loss_val: 0.1935 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0019 loss_train: 0.2964 acc_train: 0.8625 loss_val: 0.1877 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0020 loss_train: 0.2910 acc_train: 0.8875 loss_val: 0.1755 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0021 loss_train: 0.2695 acc_train: 0.8250 loss_val: 0.1688 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0022 loss_train: 0.2587 acc_train: 0.9000 loss_val: 0.1700 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0023 loss_train: 0.2590 acc_train: 0.8875 loss_val: 0.1653 acc_val: 0.9200 time: 0.0020s\n",
      "Epoch: 0024 loss_train: 0.2373 acc_train: 0.9000 loss_val: 0.1646 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0025 loss_train: 0.2678 acc_train: 0.9000 loss_val: 0.1660 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0026 loss_train: 0.2784 acc_train: 0.8250 loss_val: 0.1696 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0027 loss_train: 0.2411 acc_train: 0.8750 loss_val: 0.1759 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0028 loss_train: 0.2211 acc_train: 0.8750 loss_val: 0.1880 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0029 loss_train: 0.2814 acc_train: 0.8625 loss_val: 0.1824 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0030 loss_train: 0.2332 acc_train: 0.8750 loss_val: 0.1814 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0031 loss_train: 0.2437 acc_train: 0.9000 loss_val: 0.1834 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0032 loss_train: 0.2316 acc_train: 0.9000 loss_val: 0.1886 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0033 loss_train: 0.2615 acc_train: 0.8875 loss_val: 0.1957 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0034 loss_train: 0.2336 acc_train: 0.8875 loss_val: 0.1904 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0035 loss_train: 0.2343 acc_train: 0.8875 loss_val: 0.1838 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0036 loss_train: 0.2257 acc_train: 0.9000 loss_val: 0.1956 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0037 loss_train: 0.2207 acc_train: 0.8875 loss_val: 0.2134 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0038 loss_train: 0.2187 acc_train: 0.9000 loss_val: 0.2224 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0039 loss_train: 0.2247 acc_train: 0.9000 loss_val: 0.2269 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0040 loss_train: 0.2307 acc_train: 0.9125 loss_val: 0.2195 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0041 loss_train: 0.2126 acc_train: 0.9000 loss_val: 0.2112 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0042 loss_train: 0.1867 acc_train: 0.9000 loss_val: 0.2109 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0043 loss_train: 0.1842 acc_train: 0.9000 loss_val: 0.2104 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0044 loss_train: 0.2159 acc_train: 0.9000 loss_val: 0.2145 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0045 loss_train: 0.1867 acc_train: 0.9000 loss_val: 0.2197 acc_val: 0.8400 time: 0.0016s\n",
      "Epoch: 0046 loss_train: 0.1881 acc_train: 0.9000 loss_val: 0.2213 acc_val: 0.8400 time: 0.0016s\n",
      "Epoch: 0047 loss_train: 0.2081 acc_train: 0.8625 loss_val: 0.2039 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0048 loss_train: 0.1995 acc_train: 0.9000 loss_val: 0.1993 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0049 loss_train: 0.2175 acc_train: 0.8875 loss_val: 0.1948 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0050 loss_train: 0.1866 acc_train: 0.8875 loss_val: 0.1910 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0051 loss_train: 0.1931 acc_train: 0.9125 loss_val: 0.1861 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0052 loss_train: 0.1836 acc_train: 0.9125 loss_val: 0.1814 acc_val: 0.9200 time: 0.0018s\n",
      "Epoch: 0053 loss_train: 0.1844 acc_train: 0.9125 loss_val: 0.1783 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0054 loss_train: 0.1920 acc_train: 0.8875 loss_val: 0.1763 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0055 loss_train: 0.1806 acc_train: 0.9000 loss_val: 0.1770 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0056 loss_train: 0.2256 acc_train: 0.8750 loss_val: 0.1791 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0057 loss_train: 0.1909 acc_train: 0.9000 loss_val: 0.1876 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0058 loss_train: 0.2014 acc_train: 0.9000 loss_val: 0.1914 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0059 loss_train: 0.1885 acc_train: 0.8875 loss_val: 0.2023 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0060 loss_train: 0.2497 acc_train: 0.8375 loss_val: 0.1965 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0061 loss_train: 0.1885 acc_train: 0.9000 loss_val: 0.2053 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0062 loss_train: 0.1878 acc_train: 0.8750 loss_val: 0.2153 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0063 loss_train: 0.2072 acc_train: 0.9125 loss_val: 0.2176 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0064 loss_train: 0.2153 acc_train: 0.9250 loss_val: 0.2117 acc_val: 0.9200 time: 0.0019s\n",
      "Epoch: 0065 loss_train: 0.2265 acc_train: 0.8750 loss_val: 0.1996 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0066 loss_train: 0.1911 acc_train: 0.9125 loss_val: 0.1918 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0067 loss_train: 0.1950 acc_train: 0.9125 loss_val: 0.1945 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0068 loss_train: 0.1949 acc_train: 0.9000 loss_val: 0.1980 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0069 loss_train: 0.1845 acc_train: 0.9125 loss_val: 0.1974 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0070 loss_train: 0.2200 acc_train: 0.8875 loss_val: 0.1979 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0071 loss_train: 0.1825 acc_train: 0.9250 loss_val: 0.1990 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0072 loss_train: 0.1948 acc_train: 0.9125 loss_val: 0.2042 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0073 loss_train: 0.2036 acc_train: 0.8875 loss_val: 0.2082 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0074 loss_train: 0.1876 acc_train: 0.9000 loss_val: 0.2113 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0075 loss_train: 0.1854 acc_train: 0.9125 loss_val: 0.2117 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0076 loss_train: 0.2034 acc_train: 0.9125 loss_val: 0.2087 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0077 loss_train: 0.1814 acc_train: 0.9125 loss_val: 0.2081 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0078 loss_train: 0.1744 acc_train: 0.9125 loss_val: 0.2057 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0079 loss_train: 0.1753 acc_train: 0.9250 loss_val: 0.2021 acc_val: 0.8400 time: 0.0016s\n",
      "Epoch: 0080 loss_train: 0.1854 acc_train: 0.9000 loss_val: 0.1960 acc_val: 0.8800 time: 0.0021s\n",
      "Epoch: 0081 loss_train: 0.1861 acc_train: 0.9250 loss_val: 0.1860 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0082 loss_train: 0.1868 acc_train: 0.8750 loss_val: 0.1832 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0083 loss_train: 0.1803 acc_train: 0.9250 loss_val: 0.1881 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0084 loss_train: 0.1762 acc_train: 0.9125 loss_val: 0.1961 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0085 loss_train: 0.1769 acc_train: 0.9125 loss_val: 0.2026 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0086 loss_train: 0.1925 acc_train: 0.9000 loss_val: 0.2056 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0087 loss_train: 0.1764 acc_train: 0.9125 loss_val: 0.2054 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0088 loss_train: 0.1783 acc_train: 0.9125 loss_val: 0.2033 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0089 loss_train: 0.1781 acc_train: 0.9125 loss_val: 0.2011 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0090 loss_train: 0.1796 acc_train: 0.9125 loss_val: 0.2009 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0091 loss_train: 0.1679 acc_train: 0.9000 loss_val: 0.2035 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0092 loss_train: 0.1714 acc_train: 0.9250 loss_val: 0.2024 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0093 loss_train: 0.1762 acc_train: 0.9000 loss_val: 0.2010 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0094 loss_train: 0.1966 acc_train: 0.8625 loss_val: 0.1968 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0095 loss_train: 0.1695 acc_train: 0.9000 loss_val: 0.1987 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0096 loss_train: 0.1624 acc_train: 0.9125 loss_val: 0.2046 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0097 loss_train: 0.1682 acc_train: 0.9125 loss_val: 0.2158 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0098 loss_train: 0.1622 acc_train: 0.9125 loss_val: 0.2287 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0099 loss_train: 0.1981 acc_train: 0.9125 loss_val: 0.2484 acc_val: 0.8800 time: 0.0012s\n",
      "Epoch: 0100 loss_train: 0.1632 acc_train: 0.9000 loss_val: 0.2748 acc_val: 0.8400 time: 0.0016s\n",
      "Epoch: 0101 loss_train: 0.2204 acc_train: 0.9000 loss_val: 0.2191 acc_val: 0.8800 time: 0.0018s\n",
      "Epoch: 0102 loss_train: 0.1648 acc_train: 0.9250 loss_val: 0.1997 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0103 loss_train: 0.1771 acc_train: 0.9000 loss_val: 0.2077 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0104 loss_train: 0.1937 acc_train: 0.9000 loss_val: 0.2049 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0105 loss_train: 0.1718 acc_train: 0.9000 loss_val: 0.2038 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0106 loss_train: 0.1664 acc_train: 0.8875 loss_val: 0.2018 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0107 loss_train: 0.1626 acc_train: 0.9125 loss_val: 0.2033 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0108 loss_train: 0.1822 acc_train: 0.9125 loss_val: 0.2108 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0109 loss_train: 0.1663 acc_train: 0.9000 loss_val: 0.2166 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0110 loss_train: 0.1719 acc_train: 0.9000 loss_val: 0.2126 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0111 loss_train: 0.1678 acc_train: 0.8875 loss_val: 0.2104 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0112 loss_train: 0.1579 acc_train: 0.9125 loss_val: 0.2171 acc_val: 0.8400 time: 0.0017s\n",
      "Epoch: 0113 loss_train: 0.1599 acc_train: 0.9125 loss_val: 0.2541 acc_val: 0.8400 time: 0.0016s\n",
      "Epoch: 0114 loss_train: 0.1852 acc_train: 0.9250 loss_val: 0.2498 acc_val: 0.8400 time: 0.0014s\n",
      "Epoch: 0115 loss_train: 0.1809 acc_train: 0.9000 loss_val: 0.2363 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0116 loss_train: 0.1715 acc_train: 0.9125 loss_val: 0.2354 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0117 loss_train: 0.1892 acc_train: 0.9250 loss_val: 0.2434 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0118 loss_train: 0.1717 acc_train: 0.9250 loss_val: 0.2481 acc_val: 0.8800 time: 0.0013s\n",
      "Epoch: 0119 loss_train: 0.2011 acc_train: 0.9000 loss_val: 0.2495 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0120 loss_train: 0.1807 acc_train: 0.9125 loss_val: 0.2534 acc_val: 0.8800 time: 0.0012s\n",
      "Epoch: 0121 loss_train: 0.1956 acc_train: 0.8875 loss_val: 0.2670 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0122 loss_train: 0.1773 acc_train: 0.9125 loss_val: 0.2632 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0123 loss_train: 0.1790 acc_train: 0.9000 loss_val: 0.2458 acc_val: 0.8800 time: 0.0014s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0124 loss_train: 0.1804 acc_train: 0.9250 loss_val: 0.2209 acc_val: 0.8800 time: 0.0017s\n",
      "Epoch: 0125 loss_train: 0.1734 acc_train: 0.9250 loss_val: 0.2040 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0126 loss_train: 0.1634 acc_train: 0.9125 loss_val: 0.1983 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0127 loss_train: 0.1741 acc_train: 0.9000 loss_val: 0.1991 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0128 loss_train: 0.1636 acc_train: 0.9125 loss_val: 0.1999 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0129 loss_train: 0.1635 acc_train: 0.9125 loss_val: 0.2012 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0130 loss_train: 0.1605 acc_train: 0.9125 loss_val: 0.2030 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0131 loss_train: 0.1669 acc_train: 0.9000 loss_val: 0.2059 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0132 loss_train: 0.1693 acc_train: 0.9125 loss_val: 0.2096 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0133 loss_train: 0.1752 acc_train: 0.9000 loss_val: 0.2063 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0134 loss_train: 0.1723 acc_train: 0.9000 loss_val: 0.2054 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0135 loss_train: 0.1764 acc_train: 0.9125 loss_val: 0.2052 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0136 loss_train: 0.1747 acc_train: 0.9125 loss_val: 0.2057 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0137 loss_train: 0.1523 acc_train: 0.9125 loss_val: 0.2067 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0138 loss_train: 0.1588 acc_train: 0.9125 loss_val: 0.2071 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0139 loss_train: 0.1823 acc_train: 0.9125 loss_val: 0.2088 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0140 loss_train: 0.1681 acc_train: 0.9375 loss_val: 0.2086 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0141 loss_train: 0.1640 acc_train: 0.9125 loss_val: 0.2062 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0142 loss_train: 0.1658 acc_train: 0.9000 loss_val: 0.2005 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0143 loss_train: 0.1702 acc_train: 0.9125 loss_val: 0.1946 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0144 loss_train: 0.1567 acc_train: 0.9250 loss_val: 0.1890 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0145 loss_train: 0.1809 acc_train: 0.9250 loss_val: 0.1931 acc_val: 0.9200 time: 0.0017s\n",
      "Epoch: 0146 loss_train: 0.1592 acc_train: 0.9000 loss_val: 0.1964 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0147 loss_train: 0.1690 acc_train: 0.9000 loss_val: 0.1998 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0148 loss_train: 0.1676 acc_train: 0.9125 loss_val: 0.2019 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0149 loss_train: 0.1666 acc_train: 0.8875 loss_val: 0.2043 acc_val: 0.9200 time: 0.0011s\n",
      "Epoch: 0150 loss_train: 0.1617 acc_train: 0.9125 loss_val: 0.2065 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0151 loss_train: 0.1572 acc_train: 0.9000 loss_val: 0.2068 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0152 loss_train: 0.1695 acc_train: 0.9000 loss_val: 0.2049 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0153 loss_train: 0.1697 acc_train: 0.9000 loss_val: 0.2025 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0154 loss_train: 0.1546 acc_train: 0.9000 loss_val: 0.2033 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0155 loss_train: 0.1533 acc_train: 0.9250 loss_val: 0.2086 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0156 loss_train: 0.1596 acc_train: 0.9125 loss_val: 0.2182 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0157 loss_train: 0.1768 acc_train: 0.9000 loss_val: 0.2217 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0158 loss_train: 0.1560 acc_train: 0.9250 loss_val: 0.2264 acc_val: 0.9200 time: 0.0018s\n",
      "Epoch: 0159 loss_train: 0.1718 acc_train: 0.9000 loss_val: 0.2319 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0160 loss_train: 0.1605 acc_train: 0.9250 loss_val: 0.2383 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0161 loss_train: 0.1589 acc_train: 0.9125 loss_val: 0.2421 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0162 loss_train: 0.1545 acc_train: 0.9125 loss_val: 0.2444 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0163 loss_train: 0.1489 acc_train: 0.9125 loss_val: 0.2463 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0164 loss_train: 0.1749 acc_train: 0.9125 loss_val: 0.2434 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0165 loss_train: 0.1561 acc_train: 0.9125 loss_val: 0.2385 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0166 loss_train: 0.1600 acc_train: 0.9250 loss_val: 0.2300 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0167 loss_train: 0.1944 acc_train: 0.9125 loss_val: 0.2285 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0168 loss_train: 0.1677 acc_train: 0.9250 loss_val: 0.2325 acc_val: 0.8800 time: 0.0013s\n",
      "Epoch: 0169 loss_train: 0.1768 acc_train: 0.9000 loss_val: 0.2368 acc_val: 0.8800 time: 0.0013s\n",
      "Epoch: 0170 loss_train: 0.1541 acc_train: 0.9125 loss_val: 0.2375 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0171 loss_train: 0.1595 acc_train: 0.9250 loss_val: 0.2400 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0172 loss_train: 0.1645 acc_train: 0.9125 loss_val: 0.2447 acc_val: 0.8800 time: 0.0013s\n",
      "Epoch: 0173 loss_train: 0.1579 acc_train: 0.9500 loss_val: 0.2414 acc_val: 0.8400 time: 0.0014s\n",
      "Epoch: 0174 loss_train: 0.1598 acc_train: 0.9250 loss_val: 0.2267 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0175 loss_train: 0.1444 acc_train: 0.9375 loss_val: 0.2210 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0176 loss_train: 0.1586 acc_train: 0.9250 loss_val: 0.2444 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0177 loss_train: 0.1612 acc_train: 0.9125 loss_val: 0.2611 acc_val: 0.9200 time: 0.0016s\n",
      "Epoch: 0178 loss_train: 0.1745 acc_train: 0.9125 loss_val: 0.2425 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0179 loss_train: 0.1670 acc_train: 0.8875 loss_val: 0.2265 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0180 loss_train: 0.1607 acc_train: 0.9250 loss_val: 0.2256 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0181 loss_train: 0.1496 acc_train: 0.9125 loss_val: 0.2240 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0182 loss_train: 0.1392 acc_train: 0.9375 loss_val: 0.2260 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0183 loss_train: 0.1702 acc_train: 0.9125 loss_val: 0.2274 acc_val: 0.8800 time: 0.0015s\n",
      "Epoch: 0184 loss_train: 0.1626 acc_train: 0.9250 loss_val: 0.2421 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0185 loss_train: 0.2000 acc_train: 0.8875 loss_val: 0.2543 acc_val: 0.9200 time: 0.0012s\n",
      "Epoch: 0186 loss_train: 0.1585 acc_train: 0.9000 loss_val: 0.3961 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0187 loss_train: 0.1926 acc_train: 0.9000 loss_val: 0.5950 acc_val: 0.7600 time: 0.0013s\n",
      "Epoch: 0188 loss_train: 0.1689 acc_train: 0.9125 loss_val: 0.9286 acc_val: 0.7200 time: 0.0013s\n",
      "Epoch: 0189 loss_train: 0.1854 acc_train: 0.8875 loss_val: 0.4976 acc_val: 0.8400 time: 0.0015s\n",
      "Epoch: 0190 loss_train: 0.1786 acc_train: 0.9000 loss_val: 0.2988 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0191 loss_train: 0.1521 acc_train: 0.8875 loss_val: 0.2423 acc_val: 0.8800 time: 0.0016s\n",
      "Epoch: 0192 loss_train: 0.1607 acc_train: 0.9250 loss_val: 0.2501 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0193 loss_train: 0.1998 acc_train: 0.8750 loss_val: 0.2590 acc_val: 0.8800 time: 0.0014s\n",
      "Epoch: 0194 loss_train: 0.1606 acc_train: 0.9375 loss_val: 0.2677 acc_val: 0.9200 time: 0.0015s\n",
      "Epoch: 0195 loss_train: 0.1510 acc_train: 0.9250 loss_val: 0.2737 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0196 loss_train: 0.1814 acc_train: 0.8875 loss_val: 0.2751 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0197 loss_train: 0.1641 acc_train: 0.9125 loss_val: 0.2739 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0198 loss_train: 0.1700 acc_train: 0.9125 loss_val: 0.2650 acc_val: 0.9200 time: 0.0014s\n",
      "Epoch: 0199 loss_train: 0.2322 acc_train: 0.8750 loss_val: 0.2426 acc_val: 0.9200 time: 0.0013s\n",
      "Epoch: 0200 loss_train: 0.1608 acc_train: 0.9250 loss_val: 0.2270 acc_val: 0.8800 time: 0.0016s\n",
      "Model training is complete!\n",
      "Total model training time: 0.3183s\n"
     ]
    }
   ],
   "source": [
    "# Record the start time of model training\n",
    "t_start = time.time()\n",
    "# Start iterative training of the GCN model, the number of iterations is set to args.epochs\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "    \n",
    "print(\"Model training is complete!\")\n",
    "print(\"Total model training time: {:.4f}s\".format(time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a205926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.2270 accuracy= 0.8800\n"
     ]
    }
   ],
   "source": [
    "# Model test function\n",
    "def test():\n",
    "    # First mark the model as eval mode\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "    # Input the graph network input feature and the transformed adjacency matrix adj into the two-layer graph convolutional neural network GCN model, \n",
    "    and the output is obtained through forward propagation, which is the predicted probability of the classification category\n",
    "    \"\"\"\n",
    "    output = model(features, adj)\n",
    "    \"\"\"\n",
    "    Find the corresponding output probability and label according to the data index of the test set, \n",
    "    and then calculate the loss and accuracy\n",
    "    \"\"\"\n",
    "    loss_test = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc23cce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daa254f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  0,   1,   2,  ...,  98, 103, 104],\n",
       "                       [  0,   0,   0,  ..., 104, 104, 104]]),\n",
       "       values=tensor([0.0909, 0.1176, 0.1818, 0.2857, 0.1538, 0.1818, 0.1818,\n",
       "                      0.0588, 0.1818, 0.2222, 0.1538, 0.1818, 0.0952, 0.6667,\n",
       "                      0.2857, 0.1818, 0.1176, 0.0909, 0.2857, 0.1538, 0.1818,\n",
       "                      0.1176, 0.1111, 0.0690, 0.0952, 0.0571, 0.1818, 0.1818,\n",
       "                      0.1429, 0.1538, 0.1818, 0.1176, 0.1818, 0.2857, 0.0769,\n",
       "                      0.1818, 0.2857, 0.1818, 0.1176, 0.1818, 0.1538, 0.0909,\n",
       "                      0.2857, 1.0000, 0.0526, 0.0465, 0.1818, 0.0408, 0.0513,\n",
       "                      0.0426, 0.0513, 0.0444, 0.0408, 0.0952, 1.0000, 0.0303,\n",
       "                      0.0690, 0.0952, 0.0513, 0.0465, 0.0741, 0.0465, 0.0408,\n",
       "                      0.0513, 0.0465, 0.0606, 0.0426, 0.0488, 0.0513, 0.0444,\n",
       "                      0.0408, 0.0571, 0.3333, 0.0444, 0.0769, 0.0465, 0.0465,\n",
       "                      0.0408, 0.0465, 0.0426, 0.0488, 0.1111, 0.0513, 0.0444,\n",
       "                      0.0408, 0.1176, 1.0000, 0.2222, 0.0606, 0.0345, 0.0952,\n",
       "                      0.0513, 0.0465, 0.0465, 0.0408, 0.0513, 0.0606, 0.0426,\n",
       "                      0.0488, 0.0408, 0.0741, 0.0571, 0.1176, 0.2222, 0.0606,\n",
       "                      0.0690, 0.0476, 0.0513, 0.0465, 0.0465, 0.0426, 0.0408,\n",
       "                      0.0571, 0.3333, 0.0513, 0.1176, 0.3333, 0.3333, 0.0571,\n",
       "                      0.2000, 0.4000, 0.2857, 0.0606, 0.0690, 0.0952, 0.0256,\n",
       "                      0.0465, 0.0870, 0.0741, 0.0465, 0.0408, 0.0513, 0.0465,\n",
       "                      0.0606, 0.0426, 0.0488, 0.0800, 0.0444, 0.0408, 0.0952,\n",
       "                      0.0741, 0.0571, 1.0000, 0.1053, 0.0606, 0.1538, 0.0690,\n",
       "                      0.0952, 0.0513, 0.0233, 0.0870, 0.0741, 0.0465, 0.0408,\n",
       "                      0.0513, 0.0465, 0.0606, 0.0426, 0.0488, 0.0800, 0.0408,\n",
       "                      0.0952, 0.0741, 0.0741, 0.0571, 1.0000, 0.1176, 0.1538,\n",
       "                      0.1818, 0.1429, 0.1053, 0.0909, 0.0952, 0.0513, 0.0444,\n",
       "                      0.0952, 0.0513, 0.0465, 0.0435, 0.0465, 0.0408, 0.0465,\n",
       "                      0.0426, 0.0800, 0.0408, 0.0952, 0.0741, 0.0741, 1.0000,\n",
       "                      0.2000, 0.0513, 0.0444, 1.0000, 1.0000, 0.0606, 0.0513,\n",
       "                      0.0465, 0.0370, 0.0465, 0.0408, 0.0513, 0.0465, 0.0606,\n",
       "                      0.0426, 0.0488, 0.0513, 0.0444, 0.0408, 0.0606, 0.1538,\n",
       "                      0.0690, 0.0952, 0.0513, 0.0465, 0.0870, 0.0741, 0.0233,\n",
       "                      0.0408, 0.0513, 0.0465, 0.0606, 0.0426, 0.0488, 0.0800,\n",
       "                      0.0513, 0.0444, 0.0408, 0.0741, 0.0741, 0.0571, 0.1053,\n",
       "                      0.0606, 0.1538, 0.0690, 0.0513, 0.0465, 0.0870, 0.0741,\n",
       "                      0.0465, 0.0204, 0.0513, 0.0465, 0.0606, 0.0426, 0.0488,\n",
       "                      0.0952, 0.0800, 0.0513, 0.0444, 0.0408, 0.0952, 0.0741,\n",
       "                      0.0741, 0.0571, 0.1176, 0.1053, 0.0606, 0.0690, 0.0513,\n",
       "                      0.0465, 0.0741, 0.0465, 0.0408, 0.0256, 0.0465, 0.0606,\n",
       "                      0.0426, 0.0488, 0.0952, 0.0513, 0.0444, 0.0408, 0.0741,\n",
       "                      0.0571, 0.1176, 0.0606, 0.1538, 0.0513, 0.0465, 0.0870,\n",
       "                      0.0741, 0.0465, 0.0408, 0.0513, 0.0233, 0.0606, 0.0426,\n",
       "                      0.0488, 0.0952, 0.0800, 0.0513, 0.0444, 0.0408, 0.0952,\n",
       "                      0.0741, 0.0741, 0.0571, 0.0606, 0.0690, 0.0513, 0.0465,\n",
       "                      0.0741, 0.0465, 0.0408, 0.0513, 0.0465, 0.0303, 0.0426,\n",
       "                      0.0488, 0.0513, 0.0444, 0.0408, 0.0741, 0.0571, 0.1053,\n",
       "                      0.0606, 0.1538, 0.0690, 0.0952, 0.0513, 0.0465, 0.0870,\n",
       "                      0.0741, 0.0465, 0.0408, 0.0513, 0.0465, 0.0606, 0.0213,\n",
       "                      0.0488, 0.0952, 0.0800, 0.0513, 0.0444, 0.0408, 0.0741,\n",
       "                      0.0741, 0.0571, 0.0606, 0.1538, 0.0690, 0.0513, 0.0465,\n",
       "                      0.0741, 0.0465, 0.0408, 0.0513, 0.0465, 0.0606, 0.0426,\n",
       "                      0.0244, 0.0952, 0.0800, 0.0513, 0.0444, 0.0408, 0.0741,\n",
       "                      0.0741, 0.0571, 0.1818, 0.0408, 0.0513, 0.0465, 0.0426,\n",
       "                      0.0488, 0.0476, 0.0513, 0.0444, 0.0408, 0.0952, 0.0513,\n",
       "                      0.0465, 0.0870, 0.0465, 0.0408, 0.0465, 0.0426, 0.0488,\n",
       "                      0.0400, 0.0408, 0.0952, 0.0741, 0.0741, 0.1053, 0.0606,\n",
       "                      0.2222, 0.6667, 0.1818, 0.4000, 0.0741, 0.0465, 0.0408,\n",
       "                      0.0513, 0.0465, 0.0606, 0.0426, 0.0488, 0.0952, 0.0256,\n",
       "                      0.0444, 0.0408, 0.0741, 0.1176, 0.1053, 0.0606, 0.6667,\n",
       "                      0.2222, 0.0513, 0.1818, 0.4000, 0.0741, 0.0465, 0.0408,\n",
       "                      0.0513, 0.0465, 0.0606, 0.0426, 0.0488, 0.0952, 0.0513,\n",
       "                      0.0222, 0.0408, 0.0952, 0.0741, 0.0571, 0.1176, 0.1053,\n",
       "                      0.0606, 0.2222, 0.0690, 0.0952, 0.0513, 0.0465, 0.0870,\n",
       "                      0.0741, 0.0465, 0.0408, 0.0513, 0.0465, 0.0606, 0.0426,\n",
       "                      0.0488, 0.0952, 0.0800, 0.0513, 0.0444, 0.0204, 0.0741,\n",
       "                      0.0741, 0.0571, 0.1176, 0.1053, 0.0513, 0.0465, 0.1818,\n",
       "                      0.0870, 0.0408, 0.0465, 0.0952, 0.0800, 0.0444, 0.0476,\n",
       "                      0.3333, 0.1818, 0.0465, 0.0870, 0.0465, 0.0408, 0.0513,\n",
       "                      0.0465, 0.0426, 0.0488, 0.0800, 0.0513, 0.0444, 0.0408,\n",
       "                      0.0370, 0.0741, 0.4000, 0.2000, 0.2857, 1.0000, 1.0000,\n",
       "                      0.0667, 0.6667, 0.1176, 0.1053, 0.1053, 0.1176, 0.1176,\n",
       "                      0.1176, 0.1333, 0.3333, 1.0000, 0.0690, 0.0513, 0.0465,\n",
       "                      0.0870, 0.0465, 0.0408, 0.0465, 0.0606, 0.0426, 0.0488,\n",
       "                      0.0800, 0.0408, 0.0741, 0.0370, 0.2222, 0.0606, 0.0690,\n",
       "                      0.0952, 0.6667, 0.0513, 0.0465, 0.0465, 0.0408, 0.0513,\n",
       "                      0.0465, 0.0606, 0.0426, 0.0488, 0.0444, 0.0408, 0.0286,\n",
       "                      0.1176, 0.2222, 0.0408, 0.0513, 0.0513, 0.0444, 0.0408,\n",
       "                      0.0571, 0.0588, 0.2857, 0.4000, 0.4000, 0.1176, 0.1429,\n",
       "                      0.1429, 0.1053, 0.1053, 0.1176, 0.0256, 0.0571, 0.0465,\n",
       "                      0.0606, 0.0541, 0.0870, 0.1333, 0.0645, 0.0800, 0.0952,\n",
       "                      0.0645, 0.0741, 0.0952, 0.1053, 0.1053, 0.0541, 0.0541,\n",
       "                      0.0800, 0.0541, 0.0488, 0.0513, 0.0286, 0.0465, 0.0606,\n",
       "                      0.0541, 0.0870, 0.0645, 0.0800, 0.0952, 0.0645, 0.0741,\n",
       "                      0.0952, 0.0541, 0.0800, 0.0541, 0.0800, 0.0541, 0.0488,\n",
       "                      0.0513, 0.0571, 0.0233, 0.0606, 0.0541, 0.4000, 0.0870,\n",
       "                      0.0645, 0.0800, 0.0952, 0.0741, 0.2222, 0.0952, 0.1053,\n",
       "                      0.1053, 0.0541, 0.0800, 0.0541, 0.0800, 0.0541, 0.0488,\n",
       "                      0.1818, 0.0513, 0.0571, 0.0465, 0.0303, 0.0541, 0.0645,\n",
       "                      0.0800, 0.0952, 0.0645, 0.0952, 0.1053, 0.0541, 0.0800,\n",
       "                      0.0541, 0.0800, 0.0541, 0.0488, 0.0513, 0.0571, 0.0465,\n",
       "                      0.0606, 0.0270, 0.0870, 0.0645, 0.0800, 0.0952, 0.0645,\n",
       "                      0.0741, 0.0952, 0.1053, 0.0541, 0.0800, 0.0541, 0.0800,\n",
       "                      0.0541, 0.0488, 0.1333, 0.0588, 0.1053, 0.1538, 0.1053,\n",
       "                      0.1176, 0.2857, 0.1176, 0.1176, 0.1333, 0.2857, 0.1176,\n",
       "                      0.0526, 0.1538, 0.1053, 0.1176, 0.2857, 0.1176, 0.1176,\n",
       "                      0.3333, 0.0645, 0.1176, 0.1053, 0.0769, 0.1053, 0.1176,\n",
       "                      0.1176, 0.1176, 0.1333, 0.2857, 0.1176, 0.1053, 0.1538,\n",
       "                      0.0526, 0.1176, 0.2857, 0.1176, 0.1176, 0.1333, 0.2857,\n",
       "                      0.1176, 0.1053, 0.1538, 0.1053, 0.0588, 0.1176, 0.1176,\n",
       "                      1.0000, 1.0000, 0.3333, 0.0645, 0.0465, 0.2000, 0.0488,\n",
       "                      0.0513, 0.0571, 0.0465, 0.0541, 0.0435, 0.1333, 0.2222,\n",
       "                      0.0741, 0.0645, 0.0741, 0.0541, 0.0541, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 0.0513, 0.0870, 0.0667, 0.0741, 0.0645,\n",
       "                      0.0741, 0.0541, 0.0541, 1.0000, 0.0513, 0.0571, 0.0465,\n",
       "                      0.0606, 0.0541, 0.0323, 0.0800, 0.0952, 0.1176, 0.0645,\n",
       "                      0.0741, 0.0541, 0.0800, 0.0800, 0.0541, 0.0488, 1.0000,\n",
       "                      0.0870, 0.1111, 0.0741, 0.0741, 0.0541, 0.0513, 0.0571,\n",
       "                      0.0465, 0.0606, 0.0541, 0.0645, 0.0400, 0.0741, 0.1176,\n",
       "                      0.0541, 0.0800, 0.0541, 0.0488, 0.1176, 0.1053, 0.1053,\n",
       "                      0.1429, 0.0513, 0.0571, 0.0465, 0.0606, 0.0541, 0.0645,\n",
       "                      0.0476, 0.0541, 0.0800, 0.0800, 0.0488, 0.0465, 0.0870,\n",
       "                      0.1333, 0.2222, 0.0800, 0.0370, 0.1176, 0.0645, 0.0741,\n",
       "                      0.2222, 0.0541, 0.0541, 0.0541, 0.0488, 0.3333, 0.1176,\n",
       "                      0.0645, 0.0800, 0.0741, 0.6667, 0.0588, 0.0645, 0.0541,\n",
       "                      0.0800, 0.0488, 0.0513, 0.0571, 0.0606, 0.0541, 0.6667,\n",
       "                      0.6667, 0.0870, 0.1333, 0.0645, 0.0741, 0.1176, 0.0323,\n",
       "                      0.0741, 0.0541, 0.0800, 0.0541, 0.0513, 0.0571, 0.0541,\n",
       "                      0.0870, 0.1333, 0.0645, 0.2222, 0.0741, 0.0645, 0.0370,\n",
       "                      0.2222, 0.0541, 0.0541, 0.0488, 0.0465, 0.0741, 0.0741,\n",
       "                      0.1111, 0.0541, 0.0513, 0.0571, 0.0465, 0.0606, 0.0541,\n",
       "                      0.0476, 0.1053, 0.1053, 0.0541, 0.0488, 0.1818, 0.0513,\n",
       "                      0.0465, 0.0952, 0.0526, 0.1053, 0.0541, 0.0800, 0.0541,\n",
       "                      0.0488, 0.1818, 0.0513, 0.0465, 0.0606, 0.0541, 0.0952,\n",
       "                      0.1053, 0.0526, 0.0541, 0.0488, 0.1818, 0.0513, 0.0571,\n",
       "                      0.0465, 0.0606, 0.0541, 0.0645, 0.0800, 0.0952, 0.0741,\n",
       "                      0.1176, 0.0952, 0.1053, 0.1053, 0.0270, 0.0800, 0.0541,\n",
       "                      0.0800, 0.0541, 0.0488, 0.0571, 0.0465, 0.0606, 0.0541,\n",
       "                      0.0645, 0.0800, 0.0952, 0.1176, 0.1053, 0.0541, 0.0400,\n",
       "                      0.0541, 0.0488, 0.0513, 0.0571, 0.0465, 0.0606, 0.0541,\n",
       "                      0.0870, 0.1333, 0.2222, 0.0741, 0.0645, 0.0741, 0.2222,\n",
       "                      0.0541, 0.0270, 0.0800, 0.0541, 0.0488, 0.1176, 0.1176,\n",
       "                      0.0513, 0.0571, 0.0465, 0.0606, 0.0541, 0.0645, 0.0952,\n",
       "                      0.0645, 0.0541, 0.0541, 0.0400, 0.0541, 0.0488, 0.0513,\n",
       "                      0.0571, 0.0465, 0.0606, 0.0541, 0.0870, 0.1333, 0.0645,\n",
       "                      0.0800, 0.0741, 0.0645, 0.0741, 0.1053, 0.0541, 0.0800,\n",
       "                      0.0541, 0.0800, 0.0270, 0.0488, 0.0513, 0.0571, 0.0465,\n",
       "                      0.0606, 0.0541, 0.4000, 0.0645, 0.0800, 0.0952, 0.0741,\n",
       "                      0.1176, 0.0741, 0.0952, 0.1053, 0.1053, 0.0541, 0.0800,\n",
       "                      0.0541, 0.0800, 0.0541, 0.0244, 0.6667, 0.0465, 0.0952,\n",
       "                      0.1053, 0.1053, 0.0909, 0.1333, 0.1176, 0.1053, 0.1538,\n",
       "                      0.1053, 0.1176, 0.0541, 0.0588, 0.1176, 0.1333, 0.1176,\n",
       "                      0.1053, 0.1538, 0.1053, 0.1176, 0.0541, 0.1176, 0.0588]),\n",
       "       size=(105, 105), nnz=987, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98f83983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features, adj).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10ec2bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4e6e4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9054c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
