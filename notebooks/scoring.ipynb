{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ashutosh/miniconda3/envs/study/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "2022-09-05 21:15:01.474043: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2022-09-05 21:15:01.500986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz\n",
      "2022-09-05 21:15:01.502639: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55594ba92bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-09-05 21:15:01.502661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-09-05 21:15:01.502783: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "walk_length = 80\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab89",
   "metadata": {},
   "source": [
    "# POLBOOKS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "models, embs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f51f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 498.77it/s, loss=1.2]\n"
     ]
    }
   ],
   "source": [
    "from residual2vec.word2vec import Word2Vec\n",
    "k = \"degree-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length\n",
    ").fit(A)\n",
    "\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7529dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 518.99it/s, loss=1.17]\n"
     ]
    }
   ],
   "source": [
    "k = \"group-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length,\n",
    ").fit(A)\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a3521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff51c1c6510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff51c1c6510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c19ed10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c19ed10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c1aec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c1aec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff58479ef10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff58479ef10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 625ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0085 - val_loss: 0.0105\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0084 - val_loss: 0.0109\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0111\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0081 - val_loss: 0.0112\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0082 - val_loss: 0.0111\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0080 - val_loss: 0.0111\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0077 - val_loss: 0.0115\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0074 - val_loss: 0.0117\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0076 - val_loss: 0.0117\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0073 - val_loss: 0.0115\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0069 - val_loss: 0.0115\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0069 - val_loss: 0.0116\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0069 - val_loss: 0.0117\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0069 - val_loss: 0.0118\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0067 - val_loss: 0.0120\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0074 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0067 - val_loss: 0.0117\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0066 - val_loss: 0.0117\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0069 - val_loss: 0.0118\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0066 - val_loss: 0.0118\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0064 - val_loss: 0.0121\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0069 - val_loss: 0.0120\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0065 - val_loss: 0.0119\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0067 - val_loss: 0.0119\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0063 - val_loss: 0.0119\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0065 - val_loss: 0.0122\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0065 - val_loss: 0.0122\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0064 - val_loss: 0.0122\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0060 - val_loss: 0.0126\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0060 - val_loss: 0.0126\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0058 - val_loss: 0.0127\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0061 - val_loss: 0.0127\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0066 - val_loss: 0.0126\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0059 - val_loss: 0.0127\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0057 - val_loss: 0.0128\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0054 - val_loss: 0.0129\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0060 - val_loss: 0.0130\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0060 - val_loss: 0.0130\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0058 - val_loss: 0.0130\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0058 - val_loss: 0.0130\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0060 - val_loss: 0.0129\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0059 - val_loss: 0.0128\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0056 - val_loss: 0.0128\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0056 - val_loss: 0.0129\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0056 - val_loss: 0.0129\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0054 - val_loss: 0.0129\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0054 - val_loss: 0.0130\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0053 - val_loss: 0.0130\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0053 - val_loss: 0.0130\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0055 - val_loss: 0.0128\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0060 - val_loss: 0.0126\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0049 - val_loss: 0.0126\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0053 - val_loss: 0.0127\n",
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff4c8024b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff4c8024b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c19e250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff51c19e250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff4c8099a10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7ff4c8099a10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff4c81df0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff4c81df0d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.0097 - val_loss: 0.0100\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0086 - val_loss: 0.0104\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0084 - val_loss: 0.0104\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0084 - val_loss: 0.0107\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0080 - val_loss: 0.0109\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0080 - val_loss: 0.0110\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0080 - val_loss: 0.0110\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0078 - val_loss: 0.0112\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0074 - val_loss: 0.0116\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0076 - val_loss: 0.0118\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0073 - val_loss: 0.0120\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0074 - val_loss: 0.0120\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0073 - val_loss: 0.0119\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0074 - val_loss: 0.0118\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0073 - val_loss: 0.0117\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0076 - val_loss: 0.0116\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0071 - val_loss: 0.0116\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0073 - val_loss: 0.0119\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0070 - val_loss: 0.0120\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0071 - val_loss: 0.0122\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0070 - val_loss: 0.0123\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0072 - val_loss: 0.0122\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0067 - val_loss: 0.0122\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0067 - val_loss: 0.0121\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0066 - val_loss: 0.0122\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0069 - val_loss: 0.0122\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0069 - val_loss: 0.0121\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0068 - val_loss: 0.0122\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0067 - val_loss: 0.0124\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0068 - val_loss: 0.0128\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0066 - val_loss: 0.0129\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0066 - val_loss: 0.0128\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0065 - val_loss: 0.0127\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0064 - val_loss: 0.0125\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0063 - val_loss: 0.0122\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0065 - val_loss: 0.0123\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0061 - val_loss: 0.0126\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0059 - val_loss: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0060 - val_loss: 0.0129\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0061 - val_loss: 0.0128\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0058 - val_loss: 0.0127\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0054 - val_loss: 0.0127\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0058 - val_loss: 0.0126\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0057 - val_loss: 0.0126\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0058 - val_loss: 0.0127\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0059 - val_loss: 0.0129\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 0.0132\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0059 - val_loss: 0.0132\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0053 - val_loss: 0.0132\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0054 - val_loss: 0.0130\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0128\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0050 - val_loss: 0.0128\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0051 - val_loss: 0.0127\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0055 - val_loss: 0.0127\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0054 - val_loss: 0.0127\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0053 - val_loss: 0.0127\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0052 - val_loss: 0.0128\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0051 - val_loss: 0.0131\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0053 - val_loss: 0.0134\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0053 - val_loss: 0.0136\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0060 - val_loss: 0.0136\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0053 - val_loss: 0.0137\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0053 - val_loss: 0.0138\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0049 - val_loss: 0.0137\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0051 - val_loss: 0.0137\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0053 - val_loss: 0.0134\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c47eed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c47eed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c19ec10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c47eed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c47eed0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff5402b34d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff5402b34d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7140 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:25.899030: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 581ms/step - loss: 0.7053 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:26.539553: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 560ms/step - loss: 0.6670 - binary_accuracy: 0.5190\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:27.198700: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 811ms/step - loss: 0.6328 - binary_accuracy: 0.6714\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:27.897599: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 704ms/step - loss: 0.6261 - binary_accuracy: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:28.585060: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 238ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:29.180019: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7ff51c2c6090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff51c551b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7ff51c551b10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7073 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:31.239367: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 640ms/step - loss: 0.7160 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:31.915894: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 663ms/step - loss: 0.6805 - binary_accuracy: 0.5000\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:32.598208: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 703ms/step - loss: 0.6526 - binary_accuracy: 0.6000\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:33.297844: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 689ms/step - loss: 0.6239 - binary_accuracy: 0.6952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:34.013286: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 271ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff49edb34d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff49edb34d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edc82d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 21:15:34.605465: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edc82d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edc8c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edc8c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff49ea0c7d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff49ea0c7d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0089 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0083 - val_loss: 0.0099\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0094 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0082 - val_loss: 0.0100\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff48205da10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7ff48205da10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edaf6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff49edaf6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff4820515d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7ff4820515d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff453641490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7ff453641490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0086 - val_loss: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0097 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0094 - val_loss: 0.0101\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0091 - val_loss: 0.0103\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0097 - val_loss: 0.0102\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0098 - val_loss: 0.0104\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0092 - val_loss: 0.0104\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0093 - val_loss: 0.0104\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0089 - val_loss: 0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assigning_color: 100%|████████████████████████████████████████████████████████████████████████████| 105/105 [00:02<00:00, 44.37it/s]\n",
      "assigning_weights: 100%|███████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 31795.68it/s]\n",
      "assigning final weights: 100%|████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 139323.61it/s]\n"
     ]
    }
   ],
   "source": [
    "embs[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)\n",
    "embs[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ").fit(A).transform(dim=dim)\n",
    "embs['GCN'] = graph_embeddings.GCN().fit(A).transform(dim=dim)\n",
    "embs[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"graphsage\"] = graph_embeddings.GraphSage().fit(A).transform(dim=dim)\n",
    "embs[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256]).fit(A).transform(dim=dim)\n",
    "embs[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "\n",
    "embs['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d7e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-unbiased (105, 128)\n",
      "group-unbiased (105, 128)\n",
      "fairwalk (105, 128)\n",
      "fairwalk-group (105, 128)\n",
      "GCN (105, 128)\n",
      "gcn-doubleK (105, 128)\n",
      "graphsage (105, 128)\n",
      "graphsage-doubleK (105, 128)\n",
      "gat (105, 128)\n",
      "gat-doubleK (105, 128)\n",
      "crosswalk (75, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in embs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa28cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_graph(emb, n, m):\n",
    "    # choose top m edges to reconstruct the graph\n",
    "    S = emb @ emb.T\n",
    "    S = np.triu(S, k=1)\n",
    "    r, c, v = sparse.find(S)\n",
    "    idx = np.argsort(-v)[:m]\n",
    "    r, c, v = r[idx], c[idx], v[idx]\n",
    "    B = sparse.csr_matrix((v, (r, c)), shape=(n, n))\n",
    "    B = B + B.T\n",
    "    B.data = B.data * 0 + 1\n",
    "    return nx.from_scipy_sparse_matrix(B + B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa4e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rgraphs = {}\n",
    "for k, emb in embs.items():\n",
    "\n",
    "    n_edges = int(A.sum() / 2)\n",
    "    n_nodes = A.shape[0]\n",
    "    rgraphs[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b445a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46153846  0.44230768  0.22115384]\n",
      "class score:  degree-unbiased 0.375\n",
      "[-0.20192307  0.46153846 -0.5192308 ]\n",
      "class score:  group-unbiased 0.39423075\n",
      "[-0.45192307  0.44230768 -0.75961536]\n",
      "class score:  fairwalk 0.551282\n",
      "[-0.43269232  0.44230768 -0.7307692 ]\n",
      "class score:  fairwalk-group 0.5352564\n",
      "[-0.29807693  0.43269232  0.17307693]\n",
      "class score:  GCN 0.30128208\n",
      "[-0.34615386  0.46153846  0.33653846]\n",
      "class score:  gcn-doubleK 0.38141024\n",
      "[-0.3846154   0.46153846 -0.5480769 ]\n",
      "class score:  graphsage 0.4647436\n",
      "[-0.48076922  0.45192307  0.3653846 ]\n",
      "class score:  graphsage-doubleK 0.4326923\n",
      "[-0.52884614  0.44230768 -0.7980769 ]\n",
      "class score:  gat 0.5897436\n",
      "[-0.47115386  0.42307693 -0.72115386]\n",
      "class score:  gat-doubleK 0.5384615\n",
      "[-0.35576922  0.47115386  0.44230768]\n",
      "class score:  crosswalk 0.4230769\n"
     ]
    }
   ],
   "source": [
    "from utils.score import statistical_parity\n",
    "scores = {}\n",
    "for k, graph in rgraphs.items():\n",
    "    # we will have to change group ids as well.\n",
    "    edges = list(nx.to_edgelist(graph))\n",
    "    edges = pd.DataFrame({\n",
    "        'source': [i[0] for i in edges],\n",
    "        'target': [i[1] for i in edges]\n",
    "    })\n",
    "    scores[k] = statistical_parity(edges, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f86b3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0,'degree-unbiased'),\n",
       " Text(0,0,'group-unbiased'),\n",
       " Text(0,0,'fairwalk'),\n",
       " Text(0,0,'fairwalk-group'),\n",
       " Text(0,0,'GCN'),\n",
       " Text(0,0,'gcn-doubleK'),\n",
       " Text(0,0,'graphsage'),\n",
       " Text(0,0,'graphsage-doubleK'),\n",
       " Text(0,0,'gat'),\n",
       " Text(0,0,'gat-doubleK'),\n",
       " Text(0,0,'crosswalk')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61142af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.score import opportunity_difference\n",
    "# scores = {}\n",
    "# for k, graph in rgraphs.items():\n",
    "#     # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "#     scores[k] = opportunity_difference(edges, group_ids)\n",
    "#     print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4985abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc = {'figure.figsize':(15,8)})\n",
    "# ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a483cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
