{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashutosh/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n",
      "Ignore this message if you do not use Glove. Otherwise, install glove python package by 'pip install glove_python_binary' \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 09:13:50.109185: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-09-13 09:13:50.109422: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-13 09:13:50.110183: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "walk_length = 80\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab89",
   "metadata": {},
   "source": [
    "# POLBOOKS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "models, embs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f51f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 329/329 [00:01<00:00, 169.02it/s, loss=1.12]\n"
     ]
    }
   ],
   "source": [
    "from residual2vec.word2vec import Word2Vec\n",
    "k = \"degree-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length\n",
    ").fit(A)\n",
    "\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7529dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 329/329 [00:01<00:00, 187.43it/s, loss=1.13]\n"
     ]
    }
   ],
   "source": [
    "k = \"group-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length,\n",
    ").fit(A)\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a3521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f12682d2130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f12682d2130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f124be6afa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f124be6afa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 09:14:01.166089: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-09-13 09:14:01.186559: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0098 - val_loss: 0.0100\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0097 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0080 - val_loss: 0.0110\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0082 - val_loss: 0.0112\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0081 - val_loss: 0.0112\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0077 - val_loss: 0.0110\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0077 - val_loss: 0.0112\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0075 - val_loss: 0.0112\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0071 - val_loss: 0.0116\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0071 - val_loss: 0.0117\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0072 - val_loss: 0.0114\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0071 - val_loss: 0.0113\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0069 - val_loss: 0.0113\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0072 - val_loss: 0.0112\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.0071 - val_loss: 0.0113\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0069 - val_loss: 0.0115\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0067 - val_loss: 0.0116\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0068 - val_loss: 0.0117\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0066 - val_loss: 0.0117\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0066 - val_loss: 0.0117\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0068 - val_loss: 0.0117\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0064 - val_loss: 0.0119\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0069 - val_loss: 0.0119\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0068 - val_loss: 0.0120\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0067 - val_loss: 0.0122\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0065 - val_loss: 0.0124\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0069 - val_loss: 0.0117\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0064 - val_loss: 0.0116\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0063 - val_loss: 0.0116\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0064 - val_loss: 0.0116\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0060 - val_loss: 0.0117\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0060 - val_loss: 0.0118\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0059 - val_loss: 0.0118\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0063 - val_loss: 0.0120\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0059 - val_loss: 0.0122\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0057 - val_loss: 0.0122\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0058 - val_loss: 0.0123\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0058 - val_loss: 0.0129\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0059 - val_loss: 0.0129\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0056 - val_loss: 0.0127\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0057 - val_loss: 0.0125\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0056 - val_loss: 0.0124\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0056 - val_loss: 0.0124\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0060 - val_loss: 0.0127\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0060 - val_loss: 0.0128\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0050 - val_loss: 0.0128\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0051 - val_loss: 0.0128\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0096 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0083 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0083 - val_loss: 0.0108\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0083 - val_loss: 0.0111\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.0082 - val_loss: 0.0111\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0081 - val_loss: 0.0112\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0077 - val_loss: 0.0113\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0077 - val_loss: 0.0115\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0077 - val_loss: 0.0115\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.0080 - val_loss: 0.0114\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0077 - val_loss: 0.0114\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0078 - val_loss: 0.0115\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0074 - val_loss: 0.0116\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0073 - val_loss: 0.0115\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0071 - val_loss: 0.0116\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0072 - val_loss: 0.0120\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0071 - val_loss: 0.0121\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0070 - val_loss: 0.0121\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0071 - val_loss: 0.0121\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0071 - val_loss: 0.0120\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0070 - val_loss: 0.0119\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0067 - val_loss: 0.0121\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0067 - val_loss: 0.0124\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0069 - val_loss: 0.0121\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0069 - val_loss: 0.0120\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0065 - val_loss: 0.0120\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0066 - val_loss: 0.0121\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0064 - val_loss: 0.0122\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0062 - val_loss: 0.0125\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0066 - val_loss: 0.0129\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0066 - val_loss: 0.0129\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0065 - val_loss: 0.0129\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0061 - val_loss: 0.0129\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0064 - val_loss: 0.0129\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0060 - val_loss: 0.0130\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0064 - val_loss: 0.0129\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0061 - val_loss: 0.0130\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0060 - val_loss: 0.0131\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.0061 - val_loss: 0.0130\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0058 - val_loss: 0.0130\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0061 - val_loss: 0.0129\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0059 - val_loss: 0.0130\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0058 - val_loss: 0.0130\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0060 - val_loss: 0.0131\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.0057 - val_loss: 0.0132\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0059 - val_loss: 0.0131\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0060 - val_loss: 0.0133\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0062 - val_loss: 0.0135\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0059 - val_loss: 0.0135\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0056 - val_loss: 0.0136\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0053 - val_loss: 0.0137\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0058 - val_loss: 0.0138\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0057 - val_loss: 0.0139\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0052 - val_loss: 0.0138\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0055 - val_loss: 0.0138\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0057 - val_loss: 0.0137\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0056 - val_loss: 0.0136\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0055 - val_loss: 0.0136\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0053 - val_loss: 0.0136\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0053 - val_loss: 0.0135\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0055 - val_loss: 0.0133\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0056 - val_loss: 0.0132\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0057 - val_loss: 0.0132\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0053 - val_loss: 0.0133\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0055 - val_loss: 0.0134\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0054 - val_loss: 0.0136\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0052 - val_loss: 0.0137\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0055 - val_loss: 0.0137\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f12409dd430>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f12409dd430>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f12407fe6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f12407fe6d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7184 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7028 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6786 - binary_accuracy: 0.5095\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6406 - binary_accuracy: 0.6143\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 0.6366 - binary_accuracy: 0.6524\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7150 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 0.7112 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 1s 984ms/step - loss: 0.6738 - binary_accuracy: 0.5048\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6572 - binary_accuracy: 0.5810\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6434 - binary_accuracy: 0.6333\n",
      "1/1 [==============================] - 1s 617ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f12683f8c10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f12683f8c10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0086 - val_loss: 0.0098\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0086 - val_loss: 0.0098\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 401ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0089 - val_loss: 0.0103\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0082 - val_loss: 0.0101\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0082 - val_loss: 0.0102\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f123fe0d040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 403ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.0089 - val_loss: 0.0102\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0093 - val_loss: 0.0099\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0087 - val_loss: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f123d386160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assigning_color: 100%|██████████████████████████████| 105/105 [00:05<00:00, 19.51it/s]\n",
      "assigning_weights: 100%|█████████████████████████| 105/105 [00:00<00:00, 17086.40it/s]\n",
      "assigning final weights: 100%|██████████████████| 105/105 [00:00<00:00, 109308.00it/s]\n"
     ]
    }
   ],
   "source": [
    "embs[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)\n",
    "embs[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ").fit(A).transform(dim=dim)\n",
    "embs['GCN'] = graph_embeddings.GCN().fit(A).transform(dim=dim)\n",
    "embs[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"graphsage\"] = graph_embeddings.GraphSage().fit(A).transform(dim=dim)\n",
    "embs[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256]).fit(A).transform(dim=dim)\n",
    "embs[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "\n",
    "embs['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d7e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-unbiased (105, 128)\n",
      "group-unbiased (105, 128)\n",
      "fairwalk (105, 128)\n",
      "fairwalk-group (105, 128)\n",
      "GCN (105, 128)\n",
      "gcn-doubleK (105, 128)\n",
      "graphsage (105, 128)\n",
      "graphsage-doubleK (105, 128)\n",
      "gat (105, 128)\n",
      "gat-doubleK (105, 128)\n",
      "crosswalk (75, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in embs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa4e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 285489.75it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 388426.72it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 365767.86it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 557302.82it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 387369.23it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 338213.21it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 391219.98it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 518119.91it/s]\n",
      "100%|██████████████████████████████████████████| 441/441 [00:00<00:00, 1328798.90it/s]\n",
      "100%|██████████████████████████████████████████| 441/441 [00:00<00:00, 1294393.33it/s]\n",
      "100%|███████████████████████████████████████████| 441/441 [00:00<00:00, 411407.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.graph_utils import reconstruct_graph\n",
    "edges = {}\n",
    "for k, emb in embs.items():\n",
    "\n",
    "    n_edges = int(A.sum() / 2)\n",
    "    n_nodes = A.shape[0]\n",
    "    edges[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b445a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  degree-unbiased 0.07440029954132525\n",
      "class score:  group-unbiased 0.12827049944029228\n",
      "class score:  fairwalk 0.07295244151083871\n",
      "class score:  fairwalk-group 0.08841938165819044\n",
      "class score:  GCN 0.06450776332907682\n",
      "class score:  gcn-doubleK 0.04510592377550515\n",
      "class score:  graphsage 0.06269048807392288\n",
      "class score:  graphsage-doubleK 0.08337314962996367\n",
      "class score:  gat 0.08802034571681358\n",
      "class score:  gat-doubleK 0.17139330021995902\n",
      "class score:  crosswalk 0.03543599111600273\n"
     ]
    }
   ],
   "source": [
    "from utils.score import statistical_parity\n",
    "scores = {}\n",
    "for k, edge in edges.items():\n",
    "    # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "    scores[k] = statistical_parity(edge, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86b3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'degree-unbiased'),\n",
       " Text(1, 0, 'group-unbiased'),\n",
       " Text(2, 0, 'fairwalk'),\n",
       " Text(3, 0, 'fairwalk-group'),\n",
       " Text(4, 0, 'GCN'),\n",
       " Text(5, 0, 'gcn-doubleK'),\n",
       " Text(6, 0, 'graphsage'),\n",
       " Text(7, 0, 'graphsage-doubleK'),\n",
       " Text(8, 0, 'gat'),\n",
       " Text(9, 0, 'gat-doubleK'),\n",
       " Text(10, 0, 'crosswalk')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61142af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.score import statistical_parity\n",
    "# scores = {}\n",
    "# for k, edge in edges.items():\n",
    "#     # we will have to change group ids as well.\n",
    "# #     edges = list(nx.to_edgelist(graph))\n",
    "# #     edges = pd.DataFrame({|\n",
    "# #         'source': [i[0] for i in edges],\n",
    "# #         'target': [i[1] for i in edges]\n",
    "# #     })\n",
    "#     scores[k] = statistical_parity(edge, group_ids, metric='gini')\n",
    "#     print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4985abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc = {'figure.figsize':(15,8)})\n",
    "# ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016f2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
