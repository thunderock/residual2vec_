{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 17:37:59.769910: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:37:59.770960: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "walk_length = 80\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab89",
   "metadata": {},
   "source": [
    "# POLBOOKS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "models, embs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f51f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 500.03it/s, loss=1.09]\n"
     ]
    }
   ],
   "source": [
    "from residual2vec.word2vec import Word2Vec\n",
    "k = \"degree-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length\n",
    ").fit(A)\n",
    "\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7529dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 535.99it/s, loss=1.22]\n"
     ]
    }
   ],
   "source": [
    "k = \"group-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length,\n",
    ").fit(A)\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a3521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0085 - val_loss: 0.0108\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0084 - val_loss: 0.0108\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0083 - val_loss: 0.0110\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0083 - val_loss: 0.0110\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0083 - val_loss: 0.0109\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0078 - val_loss: 0.0112\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0076 - val_loss: 0.0115\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0077 - val_loss: 0.0115\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0074 - val_loss: 0.0114\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0072 - val_loss: 0.0114\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0069 - val_loss: 0.0119\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0068 - val_loss: 0.0120\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0072 - val_loss: 0.0120\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0069 - val_loss: 0.0119\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0070 - val_loss: 0.0120\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0067 - val_loss: 0.0122\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0068 - val_loss: 0.0124\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0067 - val_loss: 0.0119\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0064 - val_loss: 0.0122\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0065 - val_loss: 0.0123\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0065 - val_loss: 0.0124\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0063 - val_loss: 0.0122\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0063 - val_loss: 0.0122\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0065 - val_loss: 0.0121\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0062 - val_loss: 0.0121\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0061 - val_loss: 0.0121\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0061 - val_loss: 0.0123\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0058 - val_loss: 0.0124\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0062 - val_loss: 0.0124\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0056 - val_loss: 0.0123\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0055 - val_loss: 0.0125\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0057 - val_loss: 0.0126\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0058 - val_loss: 0.0128\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0057 - val_loss: 0.0129\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0058 - val_loss: 0.0128\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0056 - val_loss: 0.0128\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0055 - val_loss: 0.0127\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0061 - val_loss: 0.0125\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0052 - val_loss: 0.0125\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0084 - val_loss: 0.0107\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0080 - val_loss: 0.0110\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0080 - val_loss: 0.0110\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0076 - val_loss: 0.0112\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0078 - val_loss: 0.0112\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0076 - val_loss: 0.0112\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0076 - val_loss: 0.0115\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0074 - val_loss: 0.0116\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0074 - val_loss: 0.0117\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0073 - val_loss: 0.0117\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0071 - val_loss: 0.0117\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0071 - val_loss: 0.0117\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0069 - val_loss: 0.0119\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0070 - val_loss: 0.0122\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0071 - val_loss: 0.0122\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0071 - val_loss: 0.0121\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0071 - val_loss: 0.0121\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0070 - val_loss: 0.0122\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0071 - val_loss: 0.0122\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0069 - val_loss: 0.0120\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0066 - val_loss: 0.0118\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0069 - val_loss: 0.0121\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0066 - val_loss: 0.0124\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0066 - val_loss: 0.0126\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0067 - val_loss: 0.0122\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0063 - val_loss: 0.0124\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0068 - val_loss: 0.0123\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0059 - val_loss: 0.0122\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0059 - val_loss: 0.0124\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0061 - val_loss: 0.0126\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0058 - val_loss: 0.0127\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0056 - val_loss: 0.0127\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0053 - val_loss: 0.0127\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0056 - val_loss: 0.0129\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0056 - val_loss: 0.0132\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0054 - val_loss: 0.0134\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0057 - val_loss: 0.0136\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0052 - val_loss: 0.0137\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0053 - val_loss: 0.0137\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0051 - val_loss: 0.0136\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0054 - val_loss: 0.0136\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0056 - val_loss: 0.0134\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0051 - val_loss: 0.0131\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0052 - val_loss: 0.0129\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0048 - val_loss: 0.0128\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0052 - val_loss: 0.0127\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0052 - val_loss: 0.0126\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0054 - val_loss: 0.0127\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0051 - val_loss: 0.0128\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 0.7197 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.7033 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.6740 - binary_accuracy: 0.5190\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.6409 - binary_accuracy: 0.6381\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.6419 - binary_accuracy: 0.6524\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 991ms/step - loss: 0.7159 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.7093 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.6699 - binary_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.6393 - binary_accuracy: 0.6714\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.6290 - binary_accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3b1e2649e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0084 - val_loss: 0.0102\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0089 - val_loss: 0.0103\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0088 - val_loss: 0.0104\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0090 - val_loss: 0.0105\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0093 - val_loss: 0.0105\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0087 - val_loss: 0.0106\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0089 - val_loss: 0.0105\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0087 - val_loss: 0.0105\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0087 - val_loss: 0.0107\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0096 - val_loss: 0.0108\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0089 - val_loss: 0.0107\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0090 - val_loss: 0.0109\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0091 - val_loss: 0.0109\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0088 - val_loss: 0.0108\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0093 - val_loss: 0.0107\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0087 - val_loss: 0.0106\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0092 - val_loss: 0.0103\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0099 - val_loss: 0.0102\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0093 - val_loss: 0.0104\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0088 - val_loss: 0.0104\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0093 - val_loss: 0.0104\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0091 - val_loss: 0.0105\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0087 - val_loss: 0.0107\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0093 - val_loss: 0.0108\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0094 - val_loss: 0.0108\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3ae16d2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 318ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assigning_color: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:02<00:00, 45.23it/s]\n",
      "assigning_weights: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 62186.09it/s]\n",
      "assigning final weights: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 278735.39it/s]\n"
     ]
    }
   ],
   "source": [
    "embs[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)\n",
    "embs[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ").fit(A).transform(dim=dim)\n",
    "embs['GCN'] = graph_embeddings.GCN().fit(A).transform(dim=dim)\n",
    "embs[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"graphsage\"] = graph_embeddings.GraphSage().fit(A).transform(dim=dim)\n",
    "embs[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256]).fit(A).transform(dim=dim)\n",
    "embs[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "\n",
    "embs['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d7e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-unbiased (105, 128)\n",
      "group-unbiased (105, 128)\n",
      "fairwalk (105, 128)\n",
      "fairwalk-group (105, 128)\n",
      "GCN (105, 128)\n",
      "gcn-doubleK (105, 128)\n",
      "graphsage (105, 128)\n",
      "graphsage-doubleK (105, 128)\n",
      "gat (105, 128)\n",
      "gat-doubleK (105, 128)\n",
      "crosswalk (73, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in embs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa28cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa4e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2138367.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1153172.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1178145.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1191809.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1194888.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1221722.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1122383.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1178145.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1193347.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1197984.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1217701.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.graph_utils import reconstruct_graph\n",
    "edges = {}\n",
    "for k, emb in embs.items():\n",
    "\n",
    "    n_edges = int(A.sum() / 2)\n",
    "    n_nodes = A.shape[0]\n",
    "    edges[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b445a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  degree-unbiased 0.07156488578182993\n",
      "class score:  group-unbiased 0.08703552460442733\n",
      "class score:  fairwalk 0.07811813485163065\n",
      "class score:  fairwalk-group 0.0764633076920784\n",
      "class score:  GCN 0.025329289577383222\n",
      "class score:  gcn-doubleK 0.03142626231255008\n",
      "class score:  graphsage 0.08592920208466762\n",
      "class score:  graphsage-doubleK 0.07601613947430501\n",
      "class score:  gat 0.07817307622909085\n",
      "class score:  gat-doubleK 0.1327359205375776\n",
      "class score:  crosswalk 0.024907086508114473\n"
     ]
    }
   ],
   "source": [
    "from utils.score import statistical_parity\n",
    "scores = {}\n",
    "for k, edges in edges.items():\n",
    "    # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "    scores[k] = statistical_parity(edges, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86b3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'degree-unbiased'),\n",
       " Text(1, 0, 'group-unbiased'),\n",
       " Text(2, 0, 'fairwalk'),\n",
       " Text(3, 0, 'fairwalk-group'),\n",
       " Text(4, 0, 'GCN'),\n",
       " Text(5, 0, 'gcn-doubleK'),\n",
       " Text(6, 0, 'graphsage'),\n",
       " Text(7, 0, 'graphsage-doubleK'),\n",
       " Text(8, 0, 'gat'),\n",
       " Text(9, 0, 'gat-doubleK'),\n",
       " Text(10, 0, 'crosswalk')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61142af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.score import opportunity_difference\n",
    "# scores = {}\n",
    "# for k, graph in rgraphs.items():\n",
    "#     # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "#     scores[k] = opportunity_difference(edges, group_ids)\n",
    "#     print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4985abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc = {'figure.figsize':(15,8)})\n",
    "# ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a483cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
