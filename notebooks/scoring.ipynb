{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 14:53:50.925991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-07 14:53:50.927354: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "walk_length = 80\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab89",
   "metadata": {},
   "source": [
    "# POLBOOKS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "models, embs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f51f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 467.52it/s, loss=1.03]\n"
     ]
    }
   ],
   "source": [
    "from residual2vec.word2vec import Word2Vec\n",
    "k = \"degree-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length\n",
    ").fit(A)\n",
    "\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7529dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 513.69it/s, loss=1.18]\n"
     ]
    }
   ],
   "source": [
    "k = \"group-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length,\n",
    ").fit(A)\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a3521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0083 - val_loss: 0.0108\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0081 - val_loss: 0.0111\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0079 - val_loss: 0.0113\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0080 - val_loss: 0.0114\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0080 - val_loss: 0.0113\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0079 - val_loss: 0.0113\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0078 - val_loss: 0.0115\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0077 - val_loss: 0.0113\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0074 - val_loss: 0.0112\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0078 - val_loss: 0.0115\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0070 - val_loss: 0.0115\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0071 - val_loss: 0.0120\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0070 - val_loss: 0.0120\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0076 - val_loss: 0.0119\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0070 - val_loss: 0.0119\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0067 - val_loss: 0.0119\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0070 - val_loss: 0.0121\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0066 - val_loss: 0.0121\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0066 - val_loss: 0.0123\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0063 - val_loss: 0.0124\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0067 - val_loss: 0.0124\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0067 - val_loss: 0.0122\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0065 - val_loss: 0.0121\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0064 - val_loss: 0.0122\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0065 - val_loss: 0.0122\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0061 - val_loss: 0.0122\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0065 - val_loss: 0.0122\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0061 - val_loss: 0.0121\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0061 - val_loss: 0.0121\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0062 - val_loss: 0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0058 - val_loss: 0.0124\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0059 - val_loss: 0.0126\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0058 - val_loss: 0.0125\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0058 - val_loss: 0.0125\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0057 - val_loss: 0.0128\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0058 - val_loss: 0.0128\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0057 - val_loss: 0.0128\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0057 - val_loss: 0.0128\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0058 - val_loss: 0.0129\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0056 - val_loss: 0.0131\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0055 - val_loss: 0.0132\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0056 - val_loss: 0.0131\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0058 - val_loss: 0.0130\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0054 - val_loss: 0.0129\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0055 - val_loss: 0.0129\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0057 - val_loss: 0.0130\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0052 - val_loss: 0.0130\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "Using GCN (local pooling) filters...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 590ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0096 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0083 - val_loss: 0.0108\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0084 - val_loss: 0.0108\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0082 - val_loss: 0.0111\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0081 - val_loss: 0.0110\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0080 - val_loss: 0.0112\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0079 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0076 - val_loss: 0.0113\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0074 - val_loss: 0.0117\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0075 - val_loss: 0.0117\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0073 - val_loss: 0.0117\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0076 - val_loss: 0.0117\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0074 - val_loss: 0.0118\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0073 - val_loss: 0.0119\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0073 - val_loss: 0.0120\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0073 - val_loss: 0.0123\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0071 - val_loss: 0.0125\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0072 - val_loss: 0.0124\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0075 - val_loss: 0.0120\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0069 - val_loss: 0.0118\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0070 - val_loss: 0.0119\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0070 - val_loss: 0.0119\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0071 - val_loss: 0.0120\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0070 - val_loss: 0.0120\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0067 - val_loss: 0.0121\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0071 - val_loss: 0.0122\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0070 - val_loss: 0.0122\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0065 - val_loss: 0.0122\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0066 - val_loss: 0.0122\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0065 - val_loss: 0.0123\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0066 - val_loss: 0.0122\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0068 - val_loss: 0.0123\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0066 - val_loss: 0.0122\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0070 - val_loss: 0.0122\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0062 - val_loss: 0.0125\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0066 - val_loss: 0.0127\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0065 - val_loss: 0.0128\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0062 - val_loss: 0.0124\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0062 - val_loss: 0.0125\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0066 - val_loss: 0.0125\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0062 - val_loss: 0.0126\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0061 - val_loss: 0.0125\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0062 - val_loss: 0.0124\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0062 - val_loss: 0.0122\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0064 - val_loss: 0.0122\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0059 - val_loss: 0.0122\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0064 - val_loss: 0.0123\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0059 - val_loss: 0.0124\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0061 - val_loss: 0.0124\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0057 - val_loss: 0.0124\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0057 - val_loss: 0.0126\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0058 - val_loss: 0.0126\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0055 - val_loss: 0.0127\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0059 - val_loss: 0.0128\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0059 - val_loss: 0.0127\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0053 - val_loss: 0.0126\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 834ms/step - loss: 0.7161 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.7089 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.6791 - binary_accuracy: 0.5095\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.6588 - binary_accuracy: 0.5810\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.6249 - binary_accuracy: 0.7000\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7115 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.7057 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.6833 - binary_accuracy: 0.5048\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.6569 - binary_accuracy: 0.6048\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.6434 - binary_accuracy: 0.6286\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5f47a308c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0092 - val_loss: 0.0103\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0084 - val_loss: 0.0104\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0082 - val_loss: 0.0103\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0091 - val_loss: 0.0103\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0089 - val_loss: 0.0103\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0088 - val_loss: 0.0105\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0087 - val_loss: 0.0105\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0082 - val_loss: 0.0106\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0088 - val_loss: 0.0106\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0087 - val_loss: 0.0105\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0090 - val_loss: 0.0105\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0090 - val_loss: 0.0105\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0091 - val_loss: 0.0105\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5f461c1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 313ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "assigning_color: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:02<00:00, 45.47it/s]\n",
      "assigning_weights: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 58354.57it/s]\n",
      "assigning final weights: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 278031.52it/s]\n"
     ]
    }
   ],
   "source": [
    "embs[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)\n",
    "embs[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ").fit(A).transform(dim=dim)\n",
    "embs['GCN'] = graph_embeddings.GCN().fit(A).transform(dim=dim)\n",
    "embs[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"graphsage\"] = graph_embeddings.GraphSage().fit(A).transform(dim=dim)\n",
    "embs[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256]).fit(A).transform(dim=dim)\n",
    "embs[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "\n",
    "embs['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d7e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-unbiased (105, 128)\n",
      "group-unbiased (105, 128)\n",
      "fairwalk (105, 128)\n",
      "fairwalk-group (105, 128)\n",
      "GCN (105, 128)\n",
      "gcn-doubleK (105, 128)\n",
      "graphsage (105, 128)\n",
      "graphsage-doubleK (105, 128)\n",
      "gat (105, 128)\n",
      "gat-doubleK (105, 128)\n",
      "crosswalk (77, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in embs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa28cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa4e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1936846.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2126078.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2338417.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2300607.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2347319.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2353292.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2383618.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2323728.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1735167.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 1930780.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 441/441 [00:00<00:00, 2269555.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.graph_utils import reconstruct_graph\n",
    "edges = {}\n",
    "for k, emb in embs.items():\n",
    "\n",
    "    n_edges = int(A.sum() / 2)\n",
    "    n_nodes = A.shape[0]\n",
    "    edges[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b445a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  degree-unbiased 0.0966756687228037\n",
      "class score:  group-unbiased 0.11794938685837963\n",
      "class score:  fairwalk 0.08079695887545563\n",
      "class score:  fairwalk-group 0.07681827030486559\n",
      "class score:  GCN 0.08107488332312839\n",
      "class score:  gcn-doubleK 0.03973244290905533\n",
      "class score:  graphsage 0.07708995726979787\n",
      "class score:  graphsage-doubleK 0.06971002863994458\n",
      "class score:  gat 0.11170733182675877\n",
      "class score:  gat-doubleK 0.12554810365309776\n",
      "class score:  crosswalk 0.04980936877587308\n"
     ]
    }
   ],
   "source": [
    "from utils.score import statistical_parity\n",
    "scores = {}\n",
    "for k, edges in edges.items():\n",
    "    # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "    scores[k] = statistical_parity(edges, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86b3d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'degree-unbiased'),\n",
       " Text(1, 0, 'group-unbiased'),\n",
       " Text(2, 0, 'fairwalk'),\n",
       " Text(3, 0, 'fairwalk-group'),\n",
       " Text(4, 0, 'GCN'),\n",
       " Text(5, 0, 'gcn-doubleK'),\n",
       " Text(6, 0, 'graphsage'),\n",
       " Text(7, 0, 'graphsage-doubleK'),\n",
       " Text(8, 0, 'gat'),\n",
       " Text(9, 0, 'gat-doubleK'),\n",
       " Text(10, 0, 'crosswalk')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61142af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.score import opportunity_difference\n",
    "# scores = {}\n",
    "# for k, graph in rgraphs.items():\n",
    "#     # we will have to change group ids as well.\n",
    "#     edges = list(nx.to_edgelist(graph))\n",
    "#     edges = pd.DataFrame({\n",
    "#         'source': [i[0] for i in edges],\n",
    "#         'target': [i[1] for i in edges]\n",
    "#     })\n",
    "#     scores[k] = opportunity_difference(edges, group_ids)\n",
    "#     print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4985abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc = {'figure.figsize':(15,8)})\n",
    "# ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a483cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
