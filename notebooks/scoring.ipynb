{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d1921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279080b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ashutosh/miniconda3/envs/study/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "2022-08-30 10:38:37.354685: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2022-08-30 10:38:37.379431: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz\n",
      "2022-08-30 10:38:37.380236: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560eead22720 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-08-30 10:38:37.380246: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-08-30 10:38:37.380572: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "from utils.score import statistical_parity\n",
    "\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "walk_length = 80\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab89",
   "metadata": {},
   "source": [
    "# POLBOOKS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "models, embs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f51f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 506.00it/s, loss=1.15]\n"
     ]
    }
   ],
   "source": [
    "from residual2vec.word2vec import Word2Vec\n",
    "k = \"degree-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length\n",
    ").fit(A)\n",
    "\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7529dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 512.29it/s, loss=1.2]\n"
     ]
    }
   ],
   "source": [
    "k = \"group-unbiased\"\n",
    "model = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    walk_length=walk_length,\n",
    ").fit(A)\n",
    "adjusted_num_walks = np.ceil(\n",
    "        num_walks\n",
    "        * np.maximum(\n",
    "            1,\n",
    "            model.batch_size\n",
    "            * model.miniters\n",
    "            / (model.n_nodes * num_walks * walk_length),\n",
    "        )\n",
    "    ).astype(int)\n",
    "d = rv.TripletSimpleDataset(\n",
    "        adjmat=model.adjmat,\n",
    "        group_ids=group_ids,\n",
    "        num_walks=adjusted_num_walks,\n",
    "        window_length=model.window_length,\n",
    "        noise_sampler=model.sampler,\n",
    "        padding_id=model.n_nodes,\n",
    "        walk_length=model.walk_length,\n",
    "        p=model.p,\n",
    "        q=model.q,\n",
    "        buffer_size=model.buffer_size,\n",
    "        context_window_type=model.context_window_type,\n",
    "    )\n",
    "dataloader = DataLoader(\n",
    "        d,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "models[k] = model\n",
    "m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "embs[k] = models[k].transform(model=m, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a914063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from utils.link_prediction import *\n",
    "# from dataset import triplet_dataset\n",
    "# from utils.config import *\n",
    "# k = \"degree-unbiased-small-gat\"\n",
    "# model = rv.residual2vec_sgd(\n",
    "#     noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "#     window_length=window_length,\n",
    "#     num_walks=num_walks,\n",
    "#     walk_length=walk_length\n",
    "# ).fit(A)\n",
    "# d = rv.TripletSimpleDataset(\n",
    "#         adjmat=model.adjmat,\n",
    "#         group_ids=group_ids,\n",
    "#         num_walks=adjusted_num_walks,\n",
    "#         window_length=model.window_length,\n",
    "#         noise_sampler=model.sampler,\n",
    "#         padding_id=model.n_nodes,\n",
    "#         walk_length=model.walk_length,\n",
    "#         p=model.p,\n",
    "#         q=model.q,\n",
    "#         buffer_size=model.buffer_size,\n",
    "#         context_window_type=model.context_window_type,\n",
    "#     )\n",
    "# # d = triplet_dataset.TripletPokecDataset()\n",
    "# dataloader = triplet_dataset.NeighborEdgeSampler(d, batch_size=model.batch_size, shuffle=True, num_workers=NUM_WORKERS, edge_sample_size=3)\n",
    "# models[k] = model\n",
    "# # m = GATLinkPrediction(in_channels=d.num_features, embedding_size=128, num_heads=5, num_layers=3, hidden_channels=64, num_embeddings=d.num_embeddings)\n",
    "# m = GCNLinkPrediction(in_channels=d.num_features, embedding_size=128, hidden_channels=64, num_layers=5, num_embeddings=d.num_embeddings)\n",
    "# models[k].transform(model=m, dataloader=dataloader)\n",
    "\n",
    "\n",
    "# # creating embeddings here, probably need to get approved\n",
    "# m.to(DEVICE)\n",
    "# m.eval()\n",
    "# d = rv.TripletSimpleDataset(\n",
    "#         adjmat=model.adjmat,\n",
    "#         group_ids=group_ids,\n",
    "#         num_walks=1,\n",
    "#         window_length=1,\n",
    "#         noise_sampler=model.sampler,\n",
    "#         padding_id=model.n_nodes,\n",
    "#         walk_length=1,\n",
    "#         p=model.p,\n",
    "#         q=model.q,\n",
    "#         buffer_size=model.buffer_size,\n",
    "#         context_window_type=model.context_window_type,debug=True\n",
    "#     )\n",
    "# dataloader = triplet_dataset.NeighborEdgeSampler(d, batch_size=1, shuffle=False, \n",
    "#                                                  num_workers=1, edge_sample_size=3, transforming=True)\n",
    "# emb = np.empty((len(group_ids), 128))\n",
    "# for idx, batch in enumerate(tqdm(dataloader)):\n",
    "#     with torch.no_grad():\n",
    "# #         print(idx)\n",
    "#         # emb[idx, :]= m.forward_i(batch[0]).detach().cpu().numpy()\n",
    "#         emb[idx * 1: (idx + 1) * 1, :] = m.forward_i(batch[0]).detach().cpu().numpy()\n",
    "# embs[k] = emb.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a3521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb97c1873d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb97c1873d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb9cc75ab50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb9cc75ab50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb8e046f5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb8e046f5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb9440d09d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb9440d09d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0093 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0093 - val_loss: 0.0099\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0085 - val_loss: 0.0105\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0085 - val_loss: 0.0105\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0083 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0081 - val_loss: 0.0107\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0080 - val_loss: 0.0107\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0077 - val_loss: 0.0108\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0079 - val_loss: 0.0109\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0076 - val_loss: 0.0112\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0077 - val_loss: 0.0114\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0076 - val_loss: 0.0115\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0075 - val_loss: 0.0115\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0074 - val_loss: 0.0115\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0072 - val_loss: 0.0115\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0071 - val_loss: 0.0116\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0074 - val_loss: 0.0116\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0070 - val_loss: 0.0116\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0069 - val_loss: 0.0117\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0072 - val_loss: 0.0116\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0070 - val_loss: 0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0068 - val_loss: 0.0116\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0068 - val_loss: 0.0119\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0068 - val_loss: 0.0120\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0064 - val_loss: 0.0121\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0065 - val_loss: 0.0121\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0067 - val_loss: 0.0120\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0067 - val_loss: 0.0120\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0066 - val_loss: 0.0120\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0066 - val_loss: 0.0121\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0067 - val_loss: 0.0121\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0065 - val_loss: 0.0123\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0062 - val_loss: 0.0125\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0059 - val_loss: 0.0127\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0060 - val_loss: 0.0126\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0064 - val_loss: 0.0124\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0059 - val_loss: 0.0121\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0058 - val_loss: 0.0121\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0061 - val_loss: 0.0121\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0059 - val_loss: 0.0123\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0060 - val_loss: 0.0125\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0056 - val_loss: 0.0129\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0058 - val_loss: 0.0131\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0054 - val_loss: 0.0131\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0057 - val_loss: 0.0129\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0056 - val_loss: 0.0128\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0053 - val_loss: 0.0128\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0057 - val_loss: 0.0129\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0053 - val_loss: 0.0130\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0053 - val_loss: 0.0132\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0056 - val_loss: 0.0134\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0051 - val_loss: 0.0136\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0053 - val_loss: 0.0137\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0059 - val_loss: 0.0135\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0053 - val_loss: 0.0133\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0056 - val_loss: 0.0132\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8c5f7ec90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8c5f7ec90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb97c143250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb97c143250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb8c5ff3b50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7fb8c5ff3b50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb9cc75af90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb9cc75af90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 717ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0085 - val_loss: 0.0105\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0084 - val_loss: 0.0107\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0084 - val_loss: 0.0110\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0084 - val_loss: 0.0109\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0083 - val_loss: 0.0108\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0080 - val_loss: 0.0109\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0080 - val_loss: 0.0109\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0079 - val_loss: 0.0109\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0079 - val_loss: 0.0112\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0076 - val_loss: 0.0114\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0077 - val_loss: 0.0113\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0077 - val_loss: 0.0112\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0075 - val_loss: 0.0112\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0076 - val_loss: 0.0115\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0077 - val_loss: 0.0114\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0073 - val_loss: 0.0114\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0075 - val_loss: 0.0114\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0072 - val_loss: 0.0114\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0070 - val_loss: 0.0117\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0070 - val_loss: 0.0118\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0072 - val_loss: 0.0119\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0071 - val_loss: 0.0120\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0071 - val_loss: 0.0121\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0071 - val_loss: 0.0120\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0069 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0067 - val_loss: 0.0120\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0068 - val_loss: 0.0121\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0069 - val_loss: 0.0123\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0069 - val_loss: 0.0123\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0067 - val_loss: 0.0124\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0066 - val_loss: 0.0125\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0067 - val_loss: 0.0124\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0065 - val_loss: 0.0124\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0065 - val_loss: 0.0125\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0065 - val_loss: 0.0126\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0067 - val_loss: 0.0125\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0066 - val_loss: 0.0125\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0063 - val_loss: 0.0125\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0063 - val_loss: 0.0126\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0064 - val_loss: 0.0126\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0063 - val_loss: 0.0127\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0064 - val_loss: 0.0127\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0068 - val_loss: 0.0129\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0061 - val_loss: 0.0130\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0062 - val_loss: 0.0131\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0064 - val_loss: 0.0133\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0066 - val_loss: 0.0133\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0060 - val_loss: 0.0131\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0129\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0062 - val_loss: 0.0127\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0061 - val_loss: 0.0127\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0061 - val_loss: 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0060 - val_loss: 0.0128\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0063 - val_loss: 0.0128\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0061 - val_loss: 0.0129\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0061 - val_loss: 0.0130\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0062 - val_loss: 0.0132\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0062 - val_loss: 0.0130\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0059 - val_loss: 0.0128\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0061 - val_loss: 0.0128\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0059 - val_loss: 0.0129\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0059 - val_loss: 0.0131\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0057 - val_loss: 0.0131\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0060 - val_loss: 0.0131\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0057 - val_loss: 0.0132\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0055 - val_loss: 0.0134\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0056 - val_loss: 0.0137\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0059 - val_loss: 0.0137\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0057 - val_loss: 0.0138\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0056 - val_loss: 0.0139\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0055 - val_loss: 0.0139\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0057 - val_loss: 0.0139\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0055 - val_loss: 0.0139\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0055 - val_loss: 0.0138\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0059 - val_loss: 0.0136\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0056 - val_loss: 0.0133\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c443f50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c443f50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c1ab1d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c443f50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb97c443f50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7fb9a4310410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7fb9a4310410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7180 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:00.115739: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 565ms/step - loss: 0.6954 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:00.744470: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 682ms/step - loss: 0.6647 - binary_accuracy: 0.5286\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:01.385826: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 654ms/step - loss: 0.6417 - binary_accuracy: 0.6524\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:02.040093: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.6281 - binary_accuracy: 0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:02.677841: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:03.235100: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7fb9a4156450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7fb97c4f46d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7fb97c4f46d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7083 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:05.087950: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 563ms/step - loss: 0.7126 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:05.747926: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 668ms/step - loss: 0.6760 - binary_accuracy: 0.5095\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:06.368158: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 691ms/step - loss: 0.6540 - binary_accuracy: 0.6000\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:07.031690: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 651ms/step - loss: 0.6204 - binary_accuracy: 0.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:07.696374: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 247ms/step\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8c4b6d590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8c4b6d590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b76690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 10:39:08.254921: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b76690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b76d10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b76d10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb8c00d1990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb8c00d1990>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0089 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0082 - val_loss: 0.0101\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8aa2b2510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7fb8aa2b2510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b71690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8c4b71690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8aa2af090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7fb8aa2af090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb8aa0ab210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7fb8aa0ab210>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0085 - val_loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0088 - val_loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "embs[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)\n",
    "embs[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ").fit(A).transform(dim=dim)\n",
    "embs['GCN'] = graph_embeddings.GCN().fit(A).transform(dim=dim)\n",
    "embs[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"graphsage\"] = graph_embeddings.GraphSage().fit(A).transform(dim=dim)\n",
    "embs[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "embs[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256]).fit(A).transform(dim=dim)\n",
    "embs[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2).fit(A).transform(dim=dim)\n",
    "\n",
    "embs['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks).fit(A).transform(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d4a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = \"degree_unbiased_gcn_pokec\"\n",
    "\n",
    "# import pickle as pkl\n",
    "\n",
    "# embs[K] = pkl.load(open(\"emb_{}.pkl\".format(\"degree-unbiased-gcn\"), \"rb\"))[\"degree-unbiased-gcn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d7e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree-unbiased (105, 128)\n",
      "group-unbiased (105, 128)\n",
      "fairwalk (105, 128)\n",
      "fairwalk-group (105, 128)\n",
      "GCN (105, 128)\n",
      "gcn-doubleK (105, 128)\n",
      "graphsage (105, 128)\n",
      "graphsage-doubleK (105, 128)\n",
      "gat (105, 128)\n",
      "gat-doubleK (105, 128)\n",
      "crosswalk (70, 128)\n",
      "degree_unbiased_gcn_pokec (1632803, 128)\n"
     ]
    }
   ],
   "source": [
    "for k, v in embs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa28cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_graph(emb, n, m):\n",
    "    # choose top m edges to reconstruct the graph\n",
    "    S = emb @ emb.T\n",
    "    S = np.triu(S, k=1)\n",
    "    r, c, v = sparse.find(S)\n",
    "    idx = np.argsort(-v)[:m]\n",
    "    r, c, v = r[idx], c[idx], v[idx]\n",
    "    B = sparse.csr_matrix((v, (r, c)), shape=(n, n))\n",
    "    B = B + B.T\n",
    "    B.data = B.data * 0 + 1\n",
    "    return nx.from_scipy_sparse_matrix(B + B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa4e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rgraphs = {}\n",
    "for k, emb in embs.items():\n",
    "#     if k == K:\n",
    "#         n_edges = 30622564 // 2\n",
    "#         n_nodes = emb.shape[0]\n",
    "#         continue\n",
    "#     else:\n",
    "    n_edges = int(A.sum() / 2)\n",
    "    n_nodes = A.shape[0]\n",
    "    rgraphs[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b445a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  degree-unbiased 0.16500354805431186\n",
      "class score:  group-unbiased 0.1096486032819944\n",
      "class score:  fairwalk 0.16511268424772294\n",
      "class score:  fairwalk-group 0.15359607604679335\n",
      "class score:  GCN 0.13983189530839035\n",
      "class score:  gcn-doubleK 0.1564731310244656\n",
      "class score:  graphsage 0.07166510992924376\n",
      "class score:  graphsage-doubleK 0.09221156182661439\n",
      "class score:  gat 0.14853003892672803\n",
      "class score:  gat-doubleK 0.19949795150658767\n",
      "class score:  crosswalk 0.08417923600464998\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for k, graph in rgraphs.items():\n",
    "    # we will have to change group ids as well.\n",
    "    scores[k] = statistical_parity(graph, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4985abb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0,'degree-unbiased'),\n",
       " Text(0,0,'group-unbiased'),\n",
       " Text(0,0,'fairwalk'),\n",
       " Text(0,0,'fairwalk-group'),\n",
       " Text(0,0,'GCN'),\n",
       " Text(0,0,'gcn-doubleK'),\n",
       " Text(0,0,'graphsage'),\n",
       " Text(0,0,'graphsage-doubleK'),\n",
       " Text(0,0,'gat'),\n",
       " Text(0,0,'gat-doubleK'),\n",
       " Text(0,0,'crosswalk')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d539127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
