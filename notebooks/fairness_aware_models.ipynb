{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ashutosh/miniconda3/envs/study/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "2022-08-14 01:55:09.342716: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2022-08-14 01:55:09.362284: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz\n",
      "2022-08-14 01:55:09.363077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8dcd7b000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-08-14 01:55:09.363087: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-08-14 01:55:09.363476: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import graph_tool.all as gt\n",
    "import graph_embeddings\n",
    "from models.crosswalk import Crosswalk\n",
    "from utils.score import statistical_parity\n",
    "import faiss\n",
    "import residual2vec as rv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_FILE = '../data/polbooks.gml'\n",
    "G = nx.read_gml(DATA_FILE)\n",
    "G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "nodes = G.nodes(data=True)\n",
    "labels, group_ids = np.unique([n[1]['value'] for n in nodes], return_inverse=True)\n",
    "\n",
    "A = nx.adjacency_matrix(G).asfptype()\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "G = nx.from_scipy_sparse_matrix(A)\n",
    "\n",
    "models = {}\n",
    "window_length = 5\n",
    "num_walks = 10\n",
    "dim = 128\n",
    "\n",
    "models[\"baseline\"] = graph_embeddings.DeepWalk(window_length=window_length, num_walks=num_walks, restart_prob=0)\n",
    "\n",
    "models[\"degree-unbiased\"] = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.ConfigModelNodeSampler(),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    cuda=True,\n",
    "    walk_length=80\n",
    ")\n",
    "\n",
    "models[\"group-unbiased\"] = rv.residual2vec_sgd(\n",
    "    noise_sampler=rv.SBMNodeSampler(\n",
    "        group_membership=group_ids, window_length=window_length,\n",
    "    ),\n",
    "    window_length=window_length,\n",
    "    num_walks=num_walks,\n",
    "    cuda=True,\n",
    "    walk_length=80,\n",
    ")\n",
    "\n",
    "models[\"fairwalk\"] = graph_embeddings.Fairwalk(window_length=window_length, num_walks=num_walks)\n",
    "models[\"fairwalk-group\"] = graph_embeddings.Fairwalk(\n",
    "    window_length=window_length, num_walks=num_walks, group_membership=group_ids\n",
    ")\n",
    "models['GCN'] = graph_embeddings.GCN()\n",
    "models[\"gcn-doubleK\"] = graph_embeddings.GCN(num_default_features=dim * 2)\n",
    "models[\"graphsage\"] = graph_embeddings.GraphSage()\n",
    "models[\"graphsage-doubleK\"] = graph_embeddings.GraphSage(num_default_features=dim * 2)\n",
    "models[\"gat\"] = graph_embeddings.GAT(layer_sizes=[64, 256])\n",
    "models[\"gat-doubleK\"] = graph_embeddings.GAT(num_default_features=dim * 2)\n",
    "\n",
    "models['crosswalk'] = Crosswalk(group_membership=group_ids, window_length=window_length, num_walks=num_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[0., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.]]),\n",
       " (105, 105))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0:].todense(), A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████                                                                                        | 1/12 [00:00<00:01,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepWalk\n",
      "residual2vec_sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                       | 0/329 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.27]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.24]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.26]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.27]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.24]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.25]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.26]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.21]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.2]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.13]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.12]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.2]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.14]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.24]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.14]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.19]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.21]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.1]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.2]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.17]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.1]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.07]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.14]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.22]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.21]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.15]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.14]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.13]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.18]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.13]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.22]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.15]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.15]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.19]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.19]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.2]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.15]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.06]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.15]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.19]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.06]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.08]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.17]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.01]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.13]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.09]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.14]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.08]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.06]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.11]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.16]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.12]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.02]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.03]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 321.97it/s, loss=1.1]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.04]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.18]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.08]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 321.97it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.13]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 365.80it/s, loss=1.1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.14]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.996]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.15]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.02]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.08]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.985]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.08]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 365.80it/s, loss=1.1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.02]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.08]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 365.80it/s, loss=1.1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.08]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.07]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.98]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.986]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.966]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.991]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.963]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.986]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.09]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.04]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.955]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.968]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████                                 | 200/329 [00:00<00:00, 365.80it/s, loss=1]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.08]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.06]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.01]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.05]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.97]\u001b[A\n",
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.983]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.02]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████▋                               | 200/329 [00:00<00:00, 365.80it/s, loss=0.987]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.02]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 365.80it/s, loss=1.03]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.03]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.06]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 300/329 [00:00<00:00, 379.38it/s, loss=0.967]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.09]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.01]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.08]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.06]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.01]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.01]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.04]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.04]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 300/329 [00:00<00:00, 379.38it/s, loss=0.985]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████████▌       | 300/329 [00:00<00:00, 379.38it/s, loss=1]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.04]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████████▌       | 300/329 [00:00<00:00, 379.38it/s, loss=1]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 300/329 [00:00<00:00, 379.38it/s, loss=0.994]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████████████████▉       | 300/329 [00:00<00:00, 379.38it/s, loss=0.966]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.07]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.06]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.08]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.07]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.05]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.05]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.05]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.01]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.04]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.11]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.03]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 379.38it/s, loss=1.04]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 333.72it/s, loss=1.16]\u001b[A\n",
      " 17%|████████████████                                                                                | 2/12 [00:05<00:34,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual2vec_sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                       | 0/329 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.39]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.38]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.37]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.36]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.35]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.33]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.26]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.27]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.27]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.31]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.24]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                             | 0/329 [00:00<?, ?it/s, loss=1.3]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.28]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.27]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.34]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.32]\u001b[A\n",
      "  0%|                                                                                            | 0/329 [00:00<?, ?it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.32]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.32]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.32]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.24]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.24]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.32]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▉                                                         | 100/329 [00:00<00:00, 356.36it/s, loss=1.3]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.31]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.23]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.29]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.23]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.26]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.23]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.24]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.24]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.28]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.22]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.27]\u001b[A\n",
      " 30%|████████████████████████▌                                                        | 100/329 [00:00<00:00, 356.36it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.29]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.32]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 386.39it/s, loss=1.3]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 386.39it/s, loss=1.3]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.29]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.17]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 386.39it/s, loss=1.3]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.29]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.18]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.18]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.25]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▊                                | 200/329 [00:00<00:00, 386.39it/s, loss=1.2]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.18]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.26]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.21]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.28]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.22]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.23]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.27]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.24]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▏                               | 200/329 [00:00<00:00, 386.39it/s, loss=1.19]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.19]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.2]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.3]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.22]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.21]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.31]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.21]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.24]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.26]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.26]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.21]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.23]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.22]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.24]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.26]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.26]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.26]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.27]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.25]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.23]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.28]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.23]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▊       | 300/329 [00:00<00:00, 394.80it/s, loss=1.22]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 329/329 [00:00<00:00, 351.87it/s, loss=1.22]\u001b[A\n",
      " 25%|████████████████████████                                                                        | 3/12 [00:06<00:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairwalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████                                                        | 5/12 [00:07<00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairwalk\n",
      "GCN\n",
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f887e689850>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f887e689850>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f887e76f310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f887e76f310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f887e6d8550>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f887e6d8550>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f887e5f8e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f887e5f8e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0083 - val_loss: 0.0105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0080 - val_loss: 0.0109\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0078 - val_loss: 0.0108\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0079 - val_loss: 0.0108\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0079 - val_loss: 0.0108\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0075 - val_loss: 0.0109\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0076 - val_loss: 0.0109\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0076 - val_loss: 0.0109\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0074 - val_loss: 0.0109\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0075 - val_loss: 0.0110\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0076 - val_loss: 0.0110\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0074 - val_loss: 0.0109\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0073 - val_loss: 0.0110\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0073 - val_loss: 0.0111\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0072 - val_loss: 0.0117\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0071 - val_loss: 0.0118\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0073 - val_loss: 0.0116\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0072 - val_loss: 0.0117\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0074 - val_loss: 0.0116\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0071 - val_loss: 0.0114\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0071 - val_loss: 0.0113\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0070 - val_loss: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0069 - val_loss: 0.0113\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0071 - val_loss: 0.0113\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0071 - val_loss: 0.0113\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0068 - val_loss: 0.0114\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0068 - val_loss: 0.0114\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0065 - val_loss: 0.0115\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0066 - val_loss: 0.0115\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0065 - val_loss: 0.0117\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0064 - val_loss: 0.0119\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0065 - val_loss: 0.0119\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0065 - val_loss: 0.0119\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0067 - val_loss: 0.0119\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0063 - val_loss: 0.0122\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0063 - val_loss: 0.0124\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0063 - val_loss: 0.0124\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0067 - val_loss: 0.0123\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0063 - val_loss: 0.0120\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0061 - val_loss: 0.0119\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0061 - val_loss: 0.0119\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0062 - val_loss: 0.0120\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0058 - val_loss: 0.0121\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0060 - val_loss: 0.0120\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0060 - val_loss: 0.0120\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0059 - val_loss: 0.0121\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0058 - val_loss: 0.0122\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0061 - val_loss: 0.0122\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0061 - val_loss: 0.0122\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0060 - val_loss: 0.0121\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0065 - val_loss: 0.0120\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0059 - val_loss: 0.0120\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0057 - val_loss: 0.0120\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0058 - val_loss: 0.0120\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0060 - val_loss: 0.0119\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0057 - val_loss: 0.0120\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0056 - val_loss: 0.0121\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0056 - val_loss: 0.0122\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0053 - val_loss: 0.0124\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0058 - val_loss: 0.0125\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0056 - val_loss: 0.0126\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0061 - val_loss: 0.0127\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0054 - val_loss: 0.0128\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0130\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0053 - val_loss: 0.0131\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0054 - val_loss: 0.0130\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0060 - val_loss: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████                                                | 6/12 [00:14<00:19,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN\n",
      "Using GCN (local pooling) filters...\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f8840193710>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f8840193710>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f884020d090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f884020d090>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f884020d790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphConvolution.call of <stellargraph.layer.gcn.GraphConvolution object at 0x7f884020d790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f8840227250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f8840227250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0093 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0085 - val_loss: 0.0105\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0084 - val_loss: 0.0106\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0084 - val_loss: 0.0107\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0083 - val_loss: 0.0107\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0077 - val_loss: 0.0108\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0077 - val_loss: 0.0107\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0077 - val_loss: 0.0107\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0076 - val_loss: 0.0109\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0077 - val_loss: 0.0110\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0076 - val_loss: 0.0110\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0075 - val_loss: 0.0110\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0077 - val_loss: 0.0111\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0075 - val_loss: 0.0111\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0072 - val_loss: 0.0112\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0075 - val_loss: 0.0112\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0074 - val_loss: 0.0112\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0073 - val_loss: 0.0113\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0073 - val_loss: 0.0113\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0071 - val_loss: 0.0112\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0072 - val_loss: 0.0112\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0071 - val_loss: 0.0112\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0073 - val_loss: 0.0111\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0069 - val_loss: 0.0111\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0068 - val_loss: 0.0112\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0068 - val_loss: 0.0113\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0067 - val_loss: 0.0114\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0066 - val_loss: 0.0116\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0069 - val_loss: 0.0116\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0068 - val_loss: 0.0118\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0068 - val_loss: 0.0120\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0065 - val_loss: 0.0120\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0069 - val_loss: 0.0118\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0066 - val_loss: 0.0119\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0067 - val_loss: 0.0120\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0063 - val_loss: 0.0117\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0064 - val_loss: 0.0117\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0064 - val_loss: 0.0118\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0119\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0063 - val_loss: 0.0118\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0065 - val_loss: 0.0119\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0062 - val_loss: 0.0121\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0062 - val_loss: 0.0120\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0062 - val_loss: 0.0121\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0058 - val_loss: 0.0121\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0060 - val_loss: 0.0122\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0062 - val_loss: 0.0123\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0060 - val_loss: 0.0124\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0059 - val_loss: 0.0126\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0057 - val_loss: 0.0128\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0060 - val_loss: 0.0128\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0059 - val_loss: 0.0127\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0060 - val_loss: 0.0126\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0059 - val_loss: 0.0125\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0058 - val_loss: 0.0124\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0056 - val_loss: 0.0124\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0057 - val_loss: 0.0125\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0055 - val_loss: 0.0125\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0066 - val_loss: 0.0125\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0054 - val_loss: 0.0125\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0055 - val_loss: 0.0127\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0057 - val_loss: 0.0127\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0056 - val_loss: 0.0128\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0056 - val_loss: 0.0130\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0056 - val_loss: 0.0131\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0055 - val_loss: 0.0130\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0055 - val_loss: 0.0131\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0051 - val_loss: 0.0130\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0053 - val_loss: 0.0131\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0055 - val_loss: 0.0132\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0054 - val_loss: 0.0133\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0134\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0050 - val_loss: 0.0135\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0058 - val_loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████████████████████████████████████                                        | 7/12 [00:22<00:22,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49890>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f8841d49e90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f887c03b590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f887c03b590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7172 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:34.058595: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 565ms/step - loss: 0.7032 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:34.680971: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 581ms/step - loss: 0.6738 - binary_accuracy: 0.5048\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:35.356511: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 638ms/step - loss: 0.6314 - binary_accuracy: 0.6952\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:35.991892: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 685ms/step - loss: 0.6257 - binary_accuracy: 0.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:36.620881: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 176ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:37.178238: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "\r",
      " 67%|████████████████████████████████████████████████████████████████                                | 8/12 [00:27<00:18,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphSAGEAggregator.call of <stellargraph.layer.graphsage.MeanAggregator object at 0x7f88940bb5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f8830695810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method LinkEmbedding.call of <stellargraph.layer.link_inference.LinkEmbedding object at 0x7f8830695810>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7200 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:39.044355: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 589ms/step - loss: 0.7083 - binary_accuracy: 0.5000\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:39.700936: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 668ms/step - loss: 0.6849 - binary_accuracy: 0.5095\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:40.366267: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 681ms/step - loss: 0.6569 - binary_accuracy: 0.5857\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:41.012271: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.6404 - binary_accuracy: 0.6286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:41.643383: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 179ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:55:42.207188: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n",
      "\r",
      " 75%|████████████████████████████████████████████████████████████████████████                        | 9/12 [00:32<00:14,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f880a1238d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f880a1238d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f880a23f5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f880a23f5d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f880a0fc150>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f880a0fc150>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f88065ad310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f88065ad310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0091 - val_loss: 0.0096\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0090 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0082 - val_loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████▏               | 10/12 [00:42<00:13,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f87e03e7f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method SqueezedSparseConversion.call of <stellargraph.layer.misc.SqueezedSparseConversion object at 0x7f87e03e7f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f87e03e1590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f87e03e1590>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f87e03e1ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GraphAttentionSparse.call of <stellargraph.layer.graph_attention.GraphAttentionSparse object at 0x7f87e03e1ad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f87e05333d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method GatherIndices.call of <stellargraph.layer.misc.GatherIndices object at 0x7f87e05333d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1 steps, validate for 1 steps\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0096 - val_loss: 0.0097\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0094 - val_loss: 0.0098\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0090 - val_loss: 0.0098\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0088 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0089 - val_loss: 0.0098\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0087 - val_loss: 0.0100\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0094 - val_loss: 0.0101\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0091 - val_loss: 0.0099\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0095 - val_loss: 0.0100\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0094 - val_loss: 0.0101\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0085 - val_loss: 0.0103\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0090 - val_loss: 0.0103\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0091 - val_loss: 0.0103\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0086 - val_loss: 0.0103\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0089 - val_loss: 0.0104\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0094 - val_loss: 0.0105\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0094 - val_loss: 0.0107\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0095 - val_loss: 0.0107\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0099 - val_loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████████████████████        | 11/12 [00:56<00:08,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crosswalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:58<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "embs = {}\n",
    "\n",
    "for k, model in tqdm(models.items()):\n",
    "    print(model.__class__.__name__)\n",
    "#     sys.stdout = open(os.devnull, 'w')\n",
    "    \n",
    "    if \"unbiased\" in k:\n",
    "        from residual2vec.word2vec import Word2Vec\n",
    "        m = Word2Vec(vocab_size=A.shape[0] + 1, embedding_size=dim, padding_idx=A.shape[0])\n",
    "        emb = model.fit(A).transform(m)\n",
    "    else:    \n",
    "        emb = model.fit(A).transform(dim=dim)\n",
    "#     sys.stdout = sys.__stdout__\n",
    "    embs[k] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def reconstruct_graph(emb, n, m):\n",
    "    # choose top m edges to reconstruct the graph\n",
    "    S = emb @ emb.T\n",
    "    S = np.triu(S, k=1)\n",
    "    r, c, v = sparse.find(S)\n",
    "    idx = np.argsort(-v)[:m]\n",
    "    r, c, v = r[idx], c[idx], v[idx]\n",
    "    B = sparse.csr_matrix((v, (r, c)), shape=(n, n))\n",
    "    B = B + B.T\n",
    "    B.data = B.data * 0 + 1\n",
    "    return nx.from_scipy_sparse_matrix(B + B.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_edges = int(A.sum() / 2)\n",
    "n_nodes = A.shape[0]\n",
    "rgraphs = {}\n",
    "for k, emb in embs.items():\n",
    "    rgraphs[k] = reconstruct_graph(emb, n_nodes, n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class score:  baseline 0.15771529989246091\n",
      "class score:  degree-unbiased 0.14310928612263124\n",
      "class score:  group-unbiased 0.10755198280251148\n",
      "class score:  fairwalk 0.15069623931546958\n",
      "class score:  fairwalk-group 0.17069608311210605\n",
      "class score:  GCN 0.17295492486654393\n",
      "class score:  gcn-doubleK 0.1461945041376309\n",
      "class score:  graphsage 0.12827708399394763\n",
      "class score:  graphsage-doubleK 0.08765276493584036\n",
      "class score:  gat 0.1789087070855444\n",
      "class score:  gat-doubleK 0.17052140065348986\n",
      "class score:  crosswalk 0.09709847426981622\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for k, graph in rgraphs.items():\n",
    "    scores[k] = statistical_parity(graph, group_ids)\n",
    "    print(\"class score: \", k, scores[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0,'baseline'),\n",
       " Text(0,0,'degree-unbiased'),\n",
       " Text(0,0,'group-unbiased'),\n",
       " Text(0,0,'fairwalk'),\n",
       " Text(0,0,'fairwalk-group'),\n",
       " Text(0,0,'GCN'),\n",
       " Text(0,0,'gcn-doubleK'),\n",
       " Text(0,0,'graphsage'),\n",
       " Text(0,0,'graphsage-doubleK'),\n",
       " Text(0,0,'gat'),\n",
       " Text(0,0,'gat-doubleK'),\n",
       " Text(0,0,'crosswalk')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.barplot(x=list(scores.keys()), y=list(scores.values()))\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(rgraphs, open('/tmp/rgraphs.pkl', 'wb'))\n",
    "pkl.dump(group_ids, open('/tmp/group_ids.pkl', 'wb'))\n",
    "pkl.dump(embs, open('/tmp/embs.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check shapes of different properties of the r2v\n",
    "model = models['degree-unbiased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "residual2vec.residual2vec_sgd.residual2vec_sgd"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 128), (105, 128))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.in_vec.shape, model.out_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
