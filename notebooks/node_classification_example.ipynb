{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "from utils.utils import sparse_to_torch_tensor, normalize_row_wise\n",
    "from utils import score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6543d107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([105, 128]),\n",
       " torch.Tensor,\n",
       " torch.Size([105]),\n",
       " torch.Tensor,\n",
       " torch.Size([105, 105]),\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t = 'fairwalk'\n",
    "embs = pkl.load(open('/tmp/embs.pkl', 'rb'))\n",
    "rgraphs = pkl.load(open(\"/tmp/rgraphs.pkl\", 'rb'))\n",
    "labels = torch.LongTensor(pkl.load(open(\"/tmp/group_ids.pkl\", 'rb')))\n",
    "adj = nx.to_scipy_sparse_matrix(rgraphs[t])\n",
    "adj = sparse_to_torch_tensor(normalize_row_wise(adj + sp.eye(adj.shape[0])))\n",
    "features = torch.FloatTensor(embs[t])\n",
    "idx_train, idx_val = torch.LongTensor(range(80)), torch.LongTensor(range(80, 105))\n",
    "features.shape, type(features), labels.shape, type(labels), adj.shape, type(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8219c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from utils.model_utils import GraphConvolution\n",
    "from utils.node_classification import GCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0a574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7131d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Training hyperparameter configuration\n",
    "class Args:\n",
    "    no_cuda = False # Whether to use cuda/gpu\n",
    "    seed = 42 # Set random seed\n",
    "    epochs = 200 # number of iterations\n",
    "    lr = 0.01 # learning rate\n",
    "    weight_decay = 5e-4 # Learning rate decay\n",
    "    n_hid = 16 # hidden layer dimension\n",
    "    dropout = 0.5 # dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = features.shape[1]\n",
    "model = GCN(n_feat=input_features,\n",
    "            n_hids=[128,32],\n",
    "            n_classes=labels.max().item() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7b5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceeca6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss_train: 1.0168009996414185\n",
      "epoch:  1 loss_train: 0.7840081453323364\n",
      "epoch:  2 loss_train: 0.6263736486434937\n",
      "epoch:  3 loss_train: 0.46968892216682434\n",
      "epoch:  4 loss_train: 0.44425979256629944\n",
      "epoch:  5 loss_train: 0.4676939845085144\n",
      "epoch:  6 loss_train: 0.44421085715293884\n",
      "epoch:  7 loss_train: 0.3470110595226288\n",
      "epoch:  8 loss_train: 0.36984488368034363\n",
      "epoch:  9 loss_train: 0.3088047206401825\n",
      "epoch:  10 loss_train: 0.3288731575012207\n",
      "epoch:  11 loss_train: 0.31389638781547546\n",
      "epoch:  12 loss_train: 0.3135485351085663\n",
      "epoch:  13 loss_train: 0.3267526924610138\n",
      "epoch:  14 loss_train: 0.3637338876724243\n",
      "epoch:  15 loss_train: 0.2902345657348633\n",
      "epoch:  16 loss_train: 0.29765862226486206\n",
      "epoch:  17 loss_train: 0.28693172335624695\n",
      "epoch:  18 loss_train: 0.28634124994277954\n",
      "epoch:  19 loss_train: 0.2779911160469055\n",
      "epoch:  20 loss_train: 0.30668359994888306\n",
      "epoch:  21 loss_train: 0.2917424738407135\n",
      "epoch:  22 loss_train: 0.28367194533348083\n",
      "epoch:  23 loss_train: 0.2443748116493225\n",
      "epoch:  24 loss_train: 0.24744927883148193\n",
      "epoch:  25 loss_train: 0.2813173532485962\n",
      "epoch:  26 loss_train: 0.27172693610191345\n",
      "epoch:  27 loss_train: 0.2828834652900696\n",
      "epoch:  28 loss_train: 0.22913022339344025\n",
      "epoch:  29 loss_train: 0.2387382984161377\n",
      "epoch:  30 loss_train: 0.23158229887485504\n",
      "epoch:  31 loss_train: 0.26395392417907715\n",
      "epoch:  32 loss_train: 0.23716965317726135\n",
      "epoch:  33 loss_train: 0.22849097847938538\n",
      "epoch:  34 loss_train: 0.22434134781360626\n",
      "epoch:  35 loss_train: 0.23454593122005463\n",
      "epoch:  36 loss_train: 0.23494946956634521\n",
      "epoch:  37 loss_train: 0.2265152931213379\n",
      "epoch:  38 loss_train: 0.22093375027179718\n",
      "epoch:  39 loss_train: 0.2584734857082367\n",
      "epoch:  40 loss_train: 0.22316434979438782\n",
      "epoch:  41 loss_train: 0.21126970648765564\n",
      "epoch:  42 loss_train: 0.2259407341480255\n",
      "epoch:  43 loss_train: 0.21732613444328308\n",
      "epoch:  44 loss_train: 0.214206725358963\n",
      "epoch:  45 loss_train: 0.2477632462978363\n",
      "epoch:  46 loss_train: 0.20968547463417053\n",
      "epoch:  47 loss_train: 0.2296137511730194\n",
      "epoch:  48 loss_train: 0.24895955622196198\n",
      "epoch:  49 loss_train: 0.24213719367980957\n",
      "epoch:  50 loss_train: 0.21725089848041534\n",
      "epoch:  51 loss_train: 0.23920327425003052\n",
      "epoch:  52 loss_train: 0.2121080905199051\n",
      "epoch:  53 loss_train: 0.21193431317806244\n",
      "epoch:  54 loss_train: 0.19230034947395325\n",
      "epoch:  55 loss_train: 0.187642440199852\n",
      "epoch:  56 loss_train: 0.18946310877799988\n",
      "epoch:  57 loss_train: 0.18667885661125183\n",
      "epoch:  58 loss_train: 0.18634256720542908\n",
      "epoch:  59 loss_train: 0.18248654901981354\n",
      "epoch:  60 loss_train: 0.22903801500797272\n",
      "epoch:  61 loss_train: 0.19010263681411743\n",
      "epoch:  62 loss_train: 0.2056315541267395\n",
      "epoch:  63 loss_train: 0.2113395482301712\n",
      "epoch:  64 loss_train: 0.20648622512817383\n",
      "epoch:  65 loss_train: 0.21288415789604187\n",
      "epoch:  66 loss_train: 0.20649729669094086\n",
      "epoch:  67 loss_train: 0.18248002231121063\n",
      "epoch:  68 loss_train: 0.1778639703989029\n",
      "epoch:  69 loss_train: 0.18868348002433777\n",
      "epoch:  70 loss_train: 0.19230762124061584\n",
      "epoch:  71 loss_train: 0.20445981621742249\n",
      "epoch:  72 loss_train: 0.18709053099155426\n",
      "epoch:  73 loss_train: 0.17056791484355927\n",
      "epoch:  74 loss_train: 0.17979896068572998\n",
      "epoch:  75 loss_train: 0.1759704053401947\n",
      "epoch:  76 loss_train: 0.18285319209098816\n",
      "epoch:  77 loss_train: 0.26485490798950195\n",
      "epoch:  78 loss_train: 0.1915382593870163\n",
      "epoch:  79 loss_train: 0.20726217329502106\n",
      "epoch:  80 loss_train: 0.2046528160572052\n",
      "epoch:  81 loss_train: 0.1914735734462738\n",
      "epoch:  82 loss_train: 0.1837526112794876\n",
      "epoch:  83 loss_train: 0.1821792721748352\n",
      "epoch:  84 loss_train: 0.2083446979522705\n",
      "epoch:  85 loss_train: 0.2075156271457672\n",
      "epoch:  86 loss_train: 0.17853248119354248\n",
      "epoch:  87 loss_train: 0.2028435915708542\n",
      "epoch:  88 loss_train: 0.18858188390731812\n",
      "epoch:  89 loss_train: 0.18504543602466583\n",
      "epoch:  90 loss_train: 0.21260185539722443\n",
      "epoch:  91 loss_train: 0.1930483728647232\n",
      "epoch:  92 loss_train: 0.19824282824993134\n",
      "epoch:  93 loss_train: 0.1919563114643097\n",
      "epoch:  94 loss_train: 0.18201647698879242\n",
      "epoch:  95 loss_train: 0.19474253058433533\n",
      "epoch:  96 loss_train: 0.20059412717819214\n",
      "epoch:  97 loss_train: 0.1940436065196991\n",
      "epoch:  98 loss_train: 0.19388309121131897\n",
      "epoch:  99 loss_train: 0.18951767683029175\n",
      "epoch:  100 loss_train: 0.18829886615276337\n",
      "epoch:  101 loss_train: 0.18054597079753876\n",
      "epoch:  102 loss_train: 0.17027665674686432\n",
      "epoch:  103 loss_train: 0.1829662024974823\n",
      "epoch:  104 loss_train: 0.18242977559566498\n",
      "epoch:  105 loss_train: 0.15893642604351044\n",
      "epoch:  106 loss_train: 0.2028648853302002\n",
      "epoch:  107 loss_train: 0.18586333096027374\n",
      "epoch:  108 loss_train: 0.1803399622440338\n",
      "epoch:  109 loss_train: 0.17965133488178253\n",
      "epoch:  110 loss_train: 0.20407253503799438\n",
      "epoch:  111 loss_train: 0.19597415626049042\n",
      "epoch:  112 loss_train: 0.18229715526103973\n",
      "epoch:  113 loss_train: 0.1835460364818573\n",
      "epoch:  114 loss_train: 0.19026517868041992\n",
      "epoch:  115 loss_train: 0.1962336152791977\n",
      "epoch:  116 loss_train: 0.19608016312122345\n",
      "epoch:  117 loss_train: 0.1541774868965149\n",
      "epoch:  118 loss_train: 0.1804613620042801\n",
      "epoch:  119 loss_train: 0.16560515761375427\n",
      "epoch:  120 loss_train: 0.18218089640140533\n",
      "epoch:  121 loss_train: 0.16371814906597137\n",
      "epoch:  122 loss_train: 0.18934723734855652\n",
      "epoch:  123 loss_train: 0.19194160401821136\n",
      "epoch:  124 loss_train: 0.17905764281749725\n",
      "epoch:  125 loss_train: 0.17502671480178833\n",
      "epoch:  126 loss_train: 0.15997613966464996\n",
      "epoch:  127 loss_train: 0.16835269331932068\n",
      "epoch:  128 loss_train: 0.17656497657299042\n",
      "epoch:  129 loss_train: 0.17973536252975464\n",
      "epoch:  130 loss_train: 0.15747018158435822\n",
      "epoch:  131 loss_train: 0.17676903307437897\n",
      "epoch:  132 loss_train: 0.1584082692861557\n",
      "epoch:  133 loss_train: 0.20206573605537415\n",
      "epoch:  134 loss_train: 0.16992154717445374\n",
      "epoch:  135 loss_train: 0.1811063438653946\n",
      "epoch:  136 loss_train: 0.17071519792079926\n",
      "epoch:  137 loss_train: 0.18029285967350006\n",
      "epoch:  138 loss_train: 0.1698431521654129\n",
      "epoch:  139 loss_train: 0.17193228006362915\n",
      "epoch:  140 loss_train: 0.15961167216300964\n",
      "epoch:  141 loss_train: 0.17141494154930115\n",
      "epoch:  142 loss_train: 0.15840893983840942\n",
      "epoch:  143 loss_train: 0.1591956913471222\n",
      "epoch:  144 loss_train: 0.17250792682170868\n",
      "epoch:  145 loss_train: 0.1968739628791809\n",
      "epoch:  146 loss_train: 0.19106559455394745\n",
      "epoch:  147 loss_train: 0.17817191779613495\n",
      "epoch:  148 loss_train: 0.17252075672149658\n",
      "epoch:  149 loss_train: 0.16456925868988037\n",
      "epoch:  150 loss_train: 0.19810344278812408\n",
      "epoch:  151 loss_train: 0.21089479327201843\n",
      "epoch:  152 loss_train: 0.18226808309555054\n",
      "epoch:  153 loss_train: 0.17934481799602509\n",
      "epoch:  154 loss_train: 0.21112264692783356\n",
      "epoch:  155 loss_train: 0.16574347019195557\n",
      "epoch:  156 loss_train: 0.18143624067306519\n",
      "epoch:  157 loss_train: 0.18257351219654083\n",
      "epoch:  158 loss_train: 0.16074569523334503\n",
      "epoch:  159 loss_train: 0.16787265241146088\n",
      "epoch:  160 loss_train: 0.17130844295024872\n",
      "epoch:  161 loss_train: 0.17312127351760864\n",
      "epoch:  162 loss_train: 0.18664458394050598\n",
      "epoch:  163 loss_train: 0.1710996925830841\n",
      "epoch:  164 loss_train: 0.18505467474460602\n",
      "epoch:  165 loss_train: 0.1748409867286682\n",
      "epoch:  166 loss_train: 0.1818726658821106\n",
      "epoch:  167 loss_train: 0.16420814394950867\n",
      "epoch:  168 loss_train: 0.18682725727558136\n",
      "epoch:  169 loss_train: 0.164864644408226\n",
      "epoch:  170 loss_train: 0.1760798543691635\n",
      "epoch:  171 loss_train: 0.18216416239738464\n",
      "epoch:  172 loss_train: 0.1655683070421219\n",
      "epoch:  173 loss_train: 0.17987607419490814\n",
      "epoch:  174 loss_train: 0.16063721477985382\n",
      "epoch:  175 loss_train: 0.17066942155361176\n",
      "epoch:  176 loss_train: 0.18220745027065277\n",
      "epoch:  177 loss_train: 0.17379619181156158\n",
      "epoch:  178 loss_train: 0.17527827620506287\n",
      "epoch:  179 loss_train: 0.17210081219673157\n",
      "epoch:  180 loss_train: 0.1703808307647705\n",
      "epoch:  181 loss_train: 0.16583451628684998\n",
      "epoch:  182 loss_train: 0.17062167823314667\n",
      "epoch:  183 loss_train: 0.17891965806484222\n",
      "epoch:  184 loss_train: 0.17672306299209595\n",
      "epoch:  185 loss_train: 0.16816502809524536\n",
      "epoch:  186 loss_train: 0.18106667697429657\n",
      "epoch:  187 loss_train: 0.182170569896698\n",
      "epoch:  188 loss_train: 0.16817063093185425\n",
      "epoch:  189 loss_train: 0.1583889126777649\n",
      "epoch:  190 loss_train: 0.16807517409324646\n",
      "epoch:  191 loss_train: 0.203998401761055\n",
      "epoch:  192 loss_train: 0.18527700006961823\n",
      "epoch:  193 loss_train: 0.16293615102767944\n",
      "epoch:  194 loss_train: 0.17675861716270447\n",
      "epoch:  195 loss_train: 0.20091381669044495\n",
      "epoch:  196 loss_train: 0.17657224833965302\n",
      "epoch:  197 loss_train: 0.16490212082862854\n",
      "epoch:  198 loss_train: 0.15952034294605255\n",
      "epoch:  199 loss_train: 0.16992604732513428\n"
     ]
    }
   ],
   "source": [
    "y_test = model.fit(X=features, y=labels, optimizer=optimizer, adj_list=adj, log=True).transform(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a205926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.1468 accuracy= 0.9200\n"
     ]
    }
   ],
   "source": [
    "# Model test function\n",
    "def test():\n",
    "    # First mark the model as eval mode\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "    # Input the graph network input feature and the transformed adjacency matrix adj into the two-layer graph convolutional neural network GCN model, \n",
    "    and the output is obtained through forward propagation, which is the predicted probability of the classification category\n",
    "    \"\"\"\n",
    "    output = model(features, adj)\n",
    "    \"\"\"\n",
    "    Find the corresponding output probability and label according to the data index of the test set, \n",
    "    and then calculate the loss and accuracy\n",
    "    \"\"\"\n",
    "    loss_test = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_test = score.accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc23cce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f83983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features, adj).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ec2bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e6e4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9054c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
